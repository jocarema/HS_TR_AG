{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas importadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza en los datos\n",
    "* Cambiar todas las palabras de mayúsculas a minúsculas\n",
    "* Se han eliminado las '@' de @USUARIO con el fin de facilitar el etiquetado morfológico\n",
    "* Quitar los links \n",
    "* Quitar los emojis\n",
    "* Cambiar los slangs, abreviaturas y contracciones en su significado\n",
    "* Se han reemplazado todos los números por el símbolo '0'\n",
    "* Cambiar los hashtag por su palabra agresiva o odiosa\n",
    "* Quitar los signos de puntuación y quitar espacios (tabuladores, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_URL=\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\"\n",
    "\n",
    "def procesar(file, namefile):    \n",
    "    file[file.columns[1]] = [clean_text(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()   \n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USUARIO\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    text= replace_all('Dictionary/SP/SPabb.txt', text)      \n",
    "    text= replace_all('Dictionary/SP/SPslang.txt', text)\n",
    "    text= replace_all('Dictionary/SP/SPcontractions.txt', text)\n",
    "    text= remove_stopwords(text)\n",
    "    text=re.sub(\"\\d+\", \"0\", text)      \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub('(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|])', \" \",text))))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):    \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "    for i in stopwords:\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, \" \", text)\n",
    "    return text\n",
    "\n",
    "def extract_hashtag(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def change_hashtag(text):    \n",
    "    input_file_agresiva = open('Dictionary/agresivas_es.txt', 'r', encoding=\"utf8\")\n",
    "    input_file_agresiva.seek(0)\n",
    "    input_file_agresiva = input_file_agresiva.read().splitlines()\n",
    "    h = extract_hashtag(text)\n",
    "    for cadena in h:\n",
    "        for agresivo in input_file_agresiva:\n",
    "            if cadena.find(agresivo) != -1:\n",
    "                text = text.replace(\"#\"+cadena,agresivo)\n",
    "        text = text.replace(\"#\"+cadena,\"\")\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs                               \n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F1F2\"\n",
    "                               \"\\U0001F1F4\"\n",
    "                               \"\\U0001F620\"\n",
    "                               \"]+\", flags=re.UNICODE)   \n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    return text\n",
    "\n",
    "def replace_all(path, text):\n",
    "    dic = create_dictionary_words(path)    \n",
    "    for i, j in dic.items():\n",
    "        text = re.sub(r\"(^|\\s)%s(\\s|$)\" % i, \" \"+j+\" \", text)\n",
    "        # r\"\\b%s\\b\"% enables replacing by whole word matches only\n",
    "    return text\n",
    "\n",
    "def create_dictionary_words(path):\n",
    "    # create a dictionary of words-to-replace and words-to-replace-with\n",
    "    input_file = open(path, 'r', encoding=\"utf8\")\n",
    "    input_file.seek(0)\n",
    "    input_file = input_file.read().splitlines()\n",
    "    input_array = [w.strip().split('\\t') for w in input_file]\n",
    "    output_dict = dict()\n",
    "    for s in input_array:\n",
    "        output_dict[s[0]]= s[1]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer los hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'17A',\n",
       " '21DARV',\n",
       " '3Ago',\n",
       " 'ASCO',\n",
       " 'AcogidaDigna',\n",
       " 'Afregar',\n",
       " 'AgresiónManterosBcnPdV',\n",
       " 'AlCongresoPorLosJubilados',\n",
       " 'Algeciras',\n",
       " 'AsiaCentral',\n",
       " 'AsíVivimosElRacismo',\n",
       " 'AñoNuevoEnCombate',\n",
       " 'Barcelona',\n",
       " 'BatiArjona',\n",
       " 'BestBoyBand',\n",
       " 'Betis',\n",
       " 'Bienvenidos13',\n",
       " 'C',\n",
       " 'CARMENA',\n",
       " 'CNCO',\n",
       " 'CaerEnTentacion',\n",
       " 'CamilaVallejo',\n",
       " 'Canarias',\n",
       " 'Carmena',\n",
       " 'Catalunya',\n",
       " 'Ceuta',\n",
       " 'Chile',\n",
       " 'ChileParaLosChilenos',\n",
       " 'ChilePrimero',\n",
       " 'CloseBorders',\n",
       " 'Closeborders',\n",
       " 'CostaRica',\n",
       " 'CállateYfriega',\n",
       " 'DeNada',\n",
       " 'DeRegaloTeMereces',\n",
       " 'DebatePorElFuturo',\n",
       " 'DefiendeChile',\n",
       " 'DefiendeEspaña',\n",
       " 'Desmontando',\n",
       " 'DimisionInutilMarlaska',\n",
       " 'DonaldCasado',\n",
       " 'ELECCIONESGENERALESYA',\n",
       " 'ELECCIONESYA',\n",
       " 'ESP',\n",
       " 'EXCLUSIVA',\n",
       " 'ElCascabel01A',\n",
       " 'EleccionesGeneralesYA',\n",
       " 'EnTuJeta',\n",
       " 'EsMuyDeProVida',\n",
       " 'Espana',\n",
       " 'España',\n",
       " 'EspañaLoPrimero',\n",
       " 'Europa',\n",
       " 'ExatlonMx',\n",
       " 'ExperimentoHistorico',\n",
       " 'FelizDomingo',\n",
       " 'FelizLunes',\n",
       " 'FelizSabado',\n",
       " 'Femimoda',\n",
       " 'FronterasSeguras',\n",
       " 'FueraRodolfoNoriega',\n",
       " 'GrandeMarlaska',\n",
       " 'GravedadAnteTodo',\n",
       " 'HechosReales',\n",
       " 'HombreAbusado',\n",
       " 'HoyLeDoyUnfollow',\n",
       " 'Humor',\n",
       " 'Idonotwantmafias',\n",
       " 'Ilegal',\n",
       " 'IllegalImmigration',\n",
       " 'InformateEnHolaChile',\n",
       " 'Inmigración',\n",
       " 'InmigrantesNO',\n",
       " 'JuanaRivasSomosTodas',\n",
       " 'Kirguistán',\n",
       " 'KosemGranFinal',\n",
       " 'L6Nfranco',\n",
       " 'LEL',\n",
       " 'LaAcademia',\n",
       " 'LaCornisa',\n",
       " 'LaSilenciosaCAT',\n",
       " 'Lavapiés',\n",
       " 'MADRILEÑOS',\n",
       " 'MaduroAcabóConVzla',\n",
       " 'MaduroGenocida',\n",
       " 'Manteros',\n",
       " 'Marruecos',\n",
       " 'MedidasYa',\n",
       " 'Mierdadepais',\n",
       " 'MiraQuienBaila',\n",
       " 'MiraculousLadybug',\n",
       " 'MoroMierdas',\n",
       " 'Morocco',\n",
       " 'Moromierdas',\n",
       " 'Motril',\n",
       " 'MásterDeHipocresía',\n",
       " 'Nicaragua',\n",
       " 'No',\n",
       " 'NosTomanPorImbéciles',\n",
       " 'Noticias',\n",
       " 'OTGala8',\n",
       " 'OmarPrieto',\n",
       " 'Openborders',\n",
       " 'POLÍTICA',\n",
       " 'PabloCasado',\n",
       " 'PabloTrump',\n",
       " 'Pap399',\n",
       " 'PartyChilensisFtAñoNuevo',\n",
       " 'PdV06A',\n",
       " 'Pendejos',\n",
       " 'Qatar',\n",
       " 'RacismoEs',\n",
       " 'RevolucionIndestructible',\n",
       " 'Runaways',\n",
       " 'SabiasQue',\n",
       " 'SanchezDimision',\n",
       " 'SanchezDimisión',\n",
       " 'SanchezPresidenteNOelecto',\n",
       " 'SanchezVeteYa',\n",
       " 'SeamosHonestos',\n",
       " 'Sevilla',\n",
       " 'SevillaBetis',\n",
       " 'Señorllevamépronto',\n",
       " 'SinCerebro',\n",
       " 'SinDerechos',\n",
       " 'SinPolla',\n",
       " 'Siria',\n",
       " 'SocialPatriotas2022',\n",
       " 'Spain',\n",
       " 'StopInvasion',\n",
       " 'StopIslam',\n",
       " 'Sub20Fem',\n",
       " 'Sudacas',\n",
       " 'TDF2018',\n",
       " 'TODOCOYOCAN',\n",
       " 'Tanger',\n",
       " 'TeamAlan',\n",
       " 'TetasFree',\n",
       " 'TipicoDeCelosos',\n",
       " 'TodasPutas',\n",
       " 'TomaDosTazas',\n",
       " 'U20WWC',\n",
       " 'UngaUngaArmy',\n",
       " 'Urgente',\n",
       " 'VOX',\n",
       " 'Venezuela',\n",
       " 'WelcomeRefugees',\n",
       " 'YoAborte',\n",
       " 'YouNeverLovedMe',\n",
       " 'YuriIsOverParty',\n",
       " 'africanos',\n",
       " 'autónomos',\n",
       " 'banislam',\n",
       " 'camilavallejo',\n",
       " 'caminodeVenezuela',\n",
       " 'campoderefugiados',\n",
       " 'camposdeautogestión',\n",
       " 'chileprimero',\n",
       " 'deber',\n",
       " 'dedazossánchez',\n",
       " 'deporten',\n",
       " 'elcascabel06a',\n",
       " 'enamorada',\n",
       " 'escuanto',\n",
       " 'españa',\n",
       " 'españaesuna',\n",
       " 'estefinde',\n",
       " 'extranjero',\n",
       " 'extranjeros',\n",
       " 'fapza',\n",
       " 'femimoda',\n",
       " 'feminazi',\n",
       " 'feminazis',\n",
       " 'güero',\n",
       " 'hombres',\n",
       " 'iHeartAwards',\n",
       " 'ilegalmente',\n",
       " 'impromilf',\n",
       " 'inmigracion',\n",
       " 'inmigración',\n",
       " 'inmigrantes',\n",
       " 'l6nveranofranco',\n",
       " 'lanzarote',\n",
       " 'maduroesmuerte',\n",
       " 'maldición',\n",
       " 'manteros',\n",
       " 'migrants',\n",
       " 'migrates',\n",
       " 'minoviaesdelsevillayesunaputamas',\n",
       " 'minovioesgenial',\n",
       " 'morenas',\n",
       " 'moromierdas',\n",
       " 'narcopisos',\n",
       " 'nv',\n",
       " 'okupas',\n",
       " 'parejainfelizSDP',\n",
       " 'perroanimal',\n",
       " 'politivida',\n",
       " 'políticos',\n",
       " 'porn',\n",
       " 'pregunta',\n",
       " 'puralata',\n",
       " 'refugiados',\n",
       " 'resentidos',\n",
       " 'sanidaduniversal',\n",
       " 'sexooral',\n",
       " 'soberanía',\n",
       " 'stopLGTB',\n",
       " 'stopOTAN',\n",
       " 'stopUE',\n",
       " 'stopglobalizacion',\n",
       " 'stopinvasion',\n",
       " 'stopislam',\n",
       " 'todasputas',\n",
       " 'topmanta',\n",
       " 'turismofobia',\n",
       " 'twitter',\n",
       " 'venezuela',\n",
       " 'votojusto',\n",
       " 'yaesmediodia32'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "corpus_train_es = pd.read_csv('corpus/public_development_esTaskA/train_es.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_es = pd.read_csv('corpus/public_development_esTaskA/dev_es.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def lista(text):\n",
    "    lista = []\n",
    "    for w in text:\n",
    "        array = extract_hash_tags(w)\n",
    "        if array !=[]:\n",
    "            for x in array:\n",
    "                lista.append(x)\n",
    "    return lista\n",
    "\n",
    "# sacar hashtag hate text\n",
    "hate_train=corpus_train_es[corpus_train_es['HS'] != 0]\n",
    "text1 = hate_train[hate_train.columns[1]]\n",
    "lista1 = lista(text1)\n",
    "hate_dev=corpus_dev_es[corpus_dev_es['HS'] != 0]\n",
    "text2 = hate_dev[hate_dev.columns[1]]\n",
    "lista2 = lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "c = a | b\n",
    "\n",
    "# sacar hashtag aggressive text\n",
    "aggressive_train=corpus_train_es[corpus_train_es['AG'] != 0]\n",
    "text1 = aggressive_train[aggressive_train.columns[1]]\n",
    "lista1=lista(text1)\n",
    "aggressive_dev=corpus_dev_es[corpus_dev_es['AG'] != 0]\n",
    "text2 = aggressive_dev[aggressive_dev.columns[1]]\n",
    "lista2=lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "d = a | b\n",
    "\n",
    "#sacar todos los hashtag\n",
    "c | d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesando el corpus A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus A\n",
    "corpus_train_esA = pd.read_csv('corpus/public_development_esTaskA/train_es.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esA = pd.read_csv('corpus/public_development_esTaskA/dev_es.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar texto medio limpio para sacar etiquetas POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesarme(file, namefile):    \n",
    "    file[file.columns[1]] = [clean(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "def clean(text):\n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USUARIO\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    text= replace_all('Dictionary/SP/SPabb.txt', text)      \n",
    "    text= replace_all('Dictionary/SP/SPslang.txt', text)\n",
    "    text= replace_all('Dictionary/SP/SPcontractions.txt', text)\n",
    "        \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", text)))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#Guardando el corpus\n",
    "procesarme(corpus_train_esA, \"corpus/public_development_esTaskA/train_es_cPOSA.tsv\")\n",
    "procesarme(corpus_dev_esA, \"corpus/public_development_esTaskA/dev_es_cPOSA.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el texto limpio A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardando el corpus ya procesado A\n",
    "procesar(corpus_train_esA, \"corpus/public_development_esTaskA/train_es_cleanA.tsv\")\n",
    "procesar(corpus_dev_esA, \"corpus/public_development_esTaskA/dev_es_cleanA.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesando el corpus limpio A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus ya procesado super limpio A\n",
    "corpus_train_esA = pd.read_csv('corpus/public_development_esTaskA/train_es_cleanA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esA = pd.read_csv('corpus/public_development_esTaskA/dev_es_cleanA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "#corpus_train_esA[corpus_train_esA.columns[1]]\n",
    "\n",
    "train_idA = corpus_train_esA[corpus_train_esA.columns[0]]\n",
    "X_train_textA = corpus_train_esA[corpus_train_esA.columns[1]].fillna(' ')\n",
    "y_train_hsA = corpus_train_esA[corpus_train_esA.columns[2]]\n",
    "y_train_trA = corpus_train_esA[corpus_train_esA.columns[3]]\n",
    "y_train_agA = corpus_train_esA[corpus_train_esA.columns[4]]\n",
    "\n",
    "test_idA = corpus_dev_esA[corpus_train_esA.columns[0]]\n",
    "X_test_textA = corpus_dev_esA[corpus_dev_esA.columns[1]].fillna(' ')\n",
    "y_test_hsA = corpus_dev_esA[corpus_dev_esA.columns[2]]\n",
    "y_test_trA = corpus_dev_esA[corpus_dev_esA.columns[3]]\n",
    "y_test_agA = corpus_dev_esA[corpus_dev_esA.columns[4]]\n",
    "\n",
    "#leyendo el corpus medio limpio para extracción de otras caracts\n",
    "corpus_train_esCA = pd.read_csv('corpus/public_development_esTaskA/train_es_cPOSA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esCA = pd.read_csv('corpus/public_development_esTaskA/dev_es_cPOSA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_A = corpus_train_esCA[corpus_train_esCA.columns[1]].fillna(' ')\n",
    "test_A = corpus_dev_esCA[corpus_dev_esCA.columns[1]].fillna(' ')\n",
    "\n",
    "#leyendo el corpus etiqueta POS\n",
    "corpus_train_esPOSA = pd.read_csv('corpus/public_development_esTaskA/train_es_cPOSTAGA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esPOSA = pd.read_csv('corpus/public_development_esTaskA/dev_es_cPOSTAGA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_posA = corpus_train_esPOSA[corpus_train_esPOSA.columns[1]].fillna(' ')\n",
    "test_posA = corpus_dev_esPOSA[corpus_dev_esPOSA.columns[1]].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_train_esA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts character n-grams\n",
    "def charNgrams(text, n):\n",
    "    ngrams = []\n",
    "    ngrams = [text[i:i+n]+'_cng' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts word-ngrams, when n=1 is equal to bag of words\n",
    "def wordNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts pos-ngrams, when n=1 is equal to bag of pos\n",
    "def posNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [pos for pos in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'_png' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunto_agresivas = set()\n",
    "words_agresiva = open('Dictionary/agresivas_es.txt', 'r', encoding=\"utf8\")\n",
    "words_agresiva.seek(0)\n",
    "words_agresiva = words_agresiva.read().splitlines()\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "for agresiva in words_agresiva:\n",
    "    conjunto_agresivas.add(stemmer.stem(agresiva))\n",
    "\n",
    "def AggressiveNgrams(text, n):\n",
    "    n_grams = []\n",
    "    tokens = text.split(\" \")    \n",
    "    fws = []\n",
    "    for word in tokens:\n",
    "        if stemmer.stem(word) in conjunto_agresivas:\n",
    "            fws.append(word)\n",
    "    n_grams=[('_'.join(fws[i:i+n])) + \"_awn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams\n",
    "\n",
    "def lexPatterns(text):\n",
    "    patterns=[]\n",
    "    #Extracts patterns\n",
    "    for word in words_agresiva:\n",
    "        w = re.findall(word, text)\n",
    "        w = ['lex_patt' for p in w]\n",
    "        patterns.extend(w)   \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSkipgrams(text,n):\n",
    "    skipgrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    lista = list(nltk.skipgrams(text, 2, n))\n",
    "    skipgrams = [' '.join(i[0]+' '+ i[1])+'' for i in lista]\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcNgrams(text, n):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"spanish\")\n",
    "    patt=r'\\b(' + ('|'.join(re.escape(key) for key in stop_words)).lstrip('|') + r')\\b'\n",
    "    pattern = re.compile(patt)\n",
    "    text = re.sub(r'[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|]*', \"\",text)\n",
    "    #text = re.sub(r\"[\" + punctuation + \"]*\", \"\", text)\n",
    "    terms = pattern.findall(text)\n",
    "    n_grams=[('_'.join(terms[i:i+n])) + \"_fwn\" for i in range(len(terms)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simbPunctNgrams(text, n):\n",
    "    simb_punt = '.,\\/!$%?¿!¡^&*;:{}=><-_`~()”“\\'\\|'\n",
    "    lis_character = list(text)\n",
    "    fws = []\n",
    "    for c in lis_character:\n",
    "        if c in simb_punt:\n",
    "            fws.append(c)\n",
    "    n_grams=[(' '.join(fws[i:i+n])) + \"_pwn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text,pos,tfs,cn,wn,pn,an,sn,fn,sp):\n",
    "    features = []\n",
    "    \n",
    "    for n in cn:\n",
    "        if n != 0:\n",
    "            features.extend(charNgrams(text,n))\n",
    "    for n in wn:\n",
    "        if n != 0:\n",
    "            features.extend(wordNgrams(text,n))\n",
    "    for n in pn:\n",
    "        if n != 0:\n",
    "            features.extend(posNgrams(pos,n))\n",
    "    for n in an:\n",
    "        if n != 0:\n",
    "            features.extend(AggressiveNgrams(text,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(wordSkipgrams(text,n))\n",
    "    for n in fn:\n",
    "        if n!=0:\n",
    "            features.extend(funcNgrams(tfs,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(simbPunctNgrams(tfs,n))\n",
    "            \n",
    "    features.extend(lexPatterns(text))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all features in a set of 'texts' and return as a string separated with the simbol '&%$'\n",
    "def process_texts(texts,poss,textfs,cn,wn,pn,an,sn,fn,sp):\n",
    "    occurrences=defaultdict(int)\n",
    "    featuresList=[]\n",
    "    featuresDict=Counter()\n",
    "    text_pos= list(zip(texts,poss,textfs))   \n",
    "    for (text,pos,tfs) in text_pos:\n",
    "        features=extract_features(text,pos,tfs,cn,wn,pn,an,sn,fn,sp)\n",
    "        featuresDict.update(features)\n",
    "        featuresList.append('&%$'.join(features))\n",
    "    return featuresList, featuresDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para guardar el archivo de salida A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tsv(testid, predictions):    \n",
    "    d = {'id': testid, 'HS': predictions}\n",
    "    file = pd.DataFrame(data=d)  \n",
    "    file.to_csv('corpus/public_development_esTaskA/es_a.tsv', sep='\\t', encoding='utf-8', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificador(cn, wn, pn, an, sn, fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textA)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textA)\n",
    "    X_train_vectorized = vect.transform(X_train_textA)\n",
    "    '''\n",
    "\n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textA, train_posA, train_A, cn, wn, pn, an, sn, fn, sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_hsA))\n",
    "    print('\\t', 'tweets', len(X_train_textA))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_hsA) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_hsA, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_hsA, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_hsA, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_hsA) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_hsA)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_hsA)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_hsA) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_hsA)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textA)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textA, test_posA, test_A, cn, wn, pn, an, sn, fn, sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textA), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    ###### File output ########################\n",
    "    print('Writing output file')\n",
    "    output_tsv(test_idA, predictionsMnB)\n",
    "    print('- File created...', 'answers saved to file:','corpus/public_development_esTaskA/es_a.tsv')\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_hsA, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_hsA, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_hsA, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_hsA, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_hsA, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_hsA, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_hsA, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_hsA, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_hsA, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_hsA, predictionsReg))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 4469\n",
      "\t tweets 4469\n",
      "\t vocabulary size 347128\n",
      "\t class dictribution Counter({0: 2631, 1: 1838})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.7471550556350235\n",
      "10-Fold Cross-validation Linear SVC 0.7274809352914955\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 500 unknown texts\n",
      " - Predicting Test\n",
      "Writing output file\n",
      "- File created... answers saved to file: corpus/public_development_esTaskA/es_a.tsv\n",
      "elapsed time: 360.2920928001404\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.798\n",
      "\t SVC 0.762\n",
      " - F1\n",
      "\t MultinomialNB 0.774049217002237\n",
      "\t SVC 0.7076167076167076\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams=[2,3] #skipgrams n-grams\n",
    "fngrams=[2,3] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "clasificador(cnvalues, wnvalues, pnvalues, anvalues, skipgrams, fngrams, spgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
