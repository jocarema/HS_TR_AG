{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas importadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza en los datos\n",
    "* Cambiar todas las palabras de mayúsculas a minúsculas\n",
    "* Se han eliminado las '@' de @USER con el fin de facilitar el etiquetado morfológico\n",
    "* Quitar los links \n",
    "* Quitar los emojis\n",
    "* Cambiar los slangs, abreviaturas y contracciones en su significado\n",
    "* Se han reemplazado todos los números por el símbolo '0'\n",
    "* Cambiar los hashtag por su palabra agresiva o odiosa\n",
    "* Quitar los signos de puntuación y quitar espacios (tabuladores, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_URL=\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\"\n",
    "\n",
    "def procesar(file, namefile):    \n",
    "    file[file.columns[1]] = [clean_text(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "    \n",
    "def clean_text(text):\n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USER\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    #estandarizar\n",
    "    text=re.sub(\"['|´]\", \"’\", text)\n",
    "    \n",
    "    #antes de cambiar todas a minúsculas, por si estan en el diccionario de las abb y contractions\n",
    "    text= replace_all('Dictionary/EN/ENabb.txt', text)\n",
    "    #text= replace_all('Dictionary/EN/ENslang.txt', text)\n",
    "    text= replace_all('Dictionary/EN/ENcontractions.txt', text)\n",
    "    \n",
    "    text = text.lower()   \n",
    "    \n",
    "    #luego de cambiar todas a minúsculas, por si estan en el diccionario de las abb y contractions\n",
    "    text= replace_all('Dictionary/EN/ENabb.txt', text)\n",
    "    #text= replace_all('Dictionary/EN/ENslang.txt', text)\n",
    "    text= replace_all('Dictionary/EN/ENcontractions.txt', text)\n",
    "    text= remove_stopwords(text)\n",
    "    \n",
    "    text=re.sub(\"\\d+\", \"0\", text)      \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(\"(?:&gt|¤|ð|ÿ|‡|¨|¦|®)\", \" \", text) \n",
    "    text=re.sub(\"&amp\", \" and \", text) \n",
    "    text=re.sub(\"&\", \" and \", text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub('(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\|])', \" \",text))))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):    \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    for i in stopwords:\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, \"\", text)\n",
    "    return text\n",
    "\n",
    "def extract_hashtag(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def change_hashtag(text):    \n",
    "    input_file_agresiva = open('Dictionary/agresivas_en.txt', 'r', encoding=\"utf8\")\n",
    "    input_file_agresiva.seek(0)\n",
    "    input_file_agresiva = input_file_agresiva.read().splitlines()\n",
    "    h = extract_hashtag(text)\n",
    "    for cadena in h:\n",
    "        for agresivo in input_file_agresiva:\n",
    "            if cadena.find(agresivo) != -1:\n",
    "                text = text.replace(\"#\"+cadena,agresivo)\n",
    "        text = text.replace(\"#\"+cadena,\"\")\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs                               \n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F1F2\"\n",
    "                               \"\\U0001F1F4\"\n",
    "                               \"\\U0001F620\"\n",
    "                               \"]+\", flags=re.UNICODE)   \n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    return text\n",
    "\n",
    "def replace_all(path, text):\n",
    "    dic = create_dictionary_words(path)    \n",
    "    for i, j in dic.items():\n",
    "        text = re.sub(r\"(^|\\s)%s(\\s|$)\" % i, \" \"+j+\" \", text)\n",
    "        # r\"\\b%s\\b\"% enables replacing by whole word matches only\n",
    "    return text\n",
    "\n",
    "def create_dictionary_words(path):\n",
    "    # create a dictionary of words-to-replace and words-to-replace-with\n",
    "    input_file = open(path, 'r', encoding=\"utf8\")\n",
    "    input_file.seek(0)\n",
    "    input_file = input_file.read().splitlines()\n",
    "    input_array = [w.strip().split('\\t') for w in input_file]\n",
    "    output_dict = dict()\n",
    "    for s in input_array:\n",
    "        output_dict[s[0]]= s[1]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer los hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'GOVERNORS',\n",
       " 'YesAllMen',\n",
       " 'Walkaway',\n",
       " 'Border',\n",
       " 'SedThemHome',\n",
       " 'GreatAwakening',\n",
       " 'marr',\n",
       " 'SaudiArabia',\n",
       " 'NoDEMS',\n",
       " 'NRCPolitics',\n",
       " 'ncpol',\n",
       " 'OnGod',\n",
       " 'NoH1Bvisa',\n",
       " 'ProudDeplorable',\n",
       " 'JobsReoport',\n",
       " 'SOCIALIST',\n",
       " 'StopOverdoses',\n",
       " 'MandateEverify',\n",
       " 'Myanmar',\n",
       " 'NationOFislam',\n",
       " 'BanSh',\n",
       " 'BoycottNFLSponsors',\n",
       " 'Animals',\n",
       " 'DeportThemAll',\n",
       " 'Somalia',\n",
       " 'persist',\n",
       " 'ForTrump',\n",
       " 'cops',\n",
       " 'Republic',\n",
       " 'massdeportations',\n",
       " 'Refugees',\n",
       " 'LawAndOrder',\n",
       " 'DrawAndQuarter',\n",
       " 'كيك',\n",
       " 'WelfareJockeys',\n",
       " 'AsylumSeekers',\n",
       " 'NoMoreAfricans',\n",
       " 'BuildThatWal',\n",
       " 'NoDaca',\n",
       " 'KeepFamiliesTogether',\n",
       " 'PatriotsUnite',\n",
       " 'SCOTUS',\n",
       " 'Labour',\n",
       " 'Eritrea',\n",
       " 'Democrtats',\n",
       " 'PETTY',\n",
       " 'NATIONALISM',\n",
       " 'angelfamilies',\n",
       " 'Guwop',\n",
       " 'pcpo',\n",
       " 'BuildTheWallNow',\n",
       " 'EndNAFTA',\n",
       " 'NeverBeto',\n",
       " 'PoetsDay',\n",
       " 'african',\n",
       " 'NoDACADeal',\n",
       " 'SundayMorning',\n",
       " 'ForThePeople',\n",
       " 'EnoughIsEnough',\n",
       " 'SaveTheChildren',\n",
       " 'MoreOfThisPlease',\n",
       " 'TaxCutReformBill',\n",
       " 'NoJihad',\n",
       " 'TrumpWho',\n",
       " 'DefundPP',\n",
       " 'IllegalMigrants',\n",
       " 'Sodyoueu',\n",
       " 'Islam',\n",
       " 'Brexiteers',\n",
       " 'RedNationRising',\n",
       " 'Horror',\n",
       " 'Trum',\n",
       " 'Canadians',\n",
       " 'MondayMorning',\n",
       " 'totallyrealnews',\n",
       " 'UnVetted',\n",
       " 'Trump',\n",
       " 'EconomicMigrant',\n",
       " 'Remove',\n",
       " 'Israel',\n",
       " 'SeeSomethingSaySomething',\n",
       " 'SENDTHEMBACK',\n",
       " 'topoli',\n",
       " 'ThirdWorldCountry',\n",
       " 'SexTraffickimg',\n",
       " 'MorningJoe',\n",
       " 'NOAmnesty',\n",
       " 'AAPSU',\n",
       " 'WhereDoISendTheCheck',\n",
       " 'VoteDemsOut',\n",
       " 'snp',\n",
       " 'VoteDemsOUT',\n",
       " 'ExtremVetting',\n",
       " 'MigrantsHow',\n",
       " 'Merkelgeschenke',\n",
       " 'MagaOneVoice',\n",
       " 'Hungary',\n",
       " 'DeathPenalty',\n",
       " 'TorontoShooting',\n",
       " 'NastyWomen',\n",
       " 'openborders',\n",
       " 'NoVisaLotte',\n",
       " 'NOASYLU',\n",
       " 'StupidWomen',\n",
       " 'Invaders',\n",
       " 'StopInvasion',\n",
       " 'WomenSuckk',\n",
       " 'NoH2Bvisa',\n",
       " 'Danforth',\n",
       " 'feminismiscancer',\n",
       " 'SCOTUSKavanaugh',\n",
       " 'beagle',\n",
       " 'SlittyBitches',\n",
       " 'Democrat',\n",
       " 'DeportIllegalAliens',\n",
       " 'BuildTheWal',\n",
       " 'AgainstAllEnemies',\n",
       " 'VoteOutTheDems',\n",
       " 'UniParty',\n",
       " 'KavanaughConfirmationHearings',\n",
       " 'SecretSociety',\n",
       " 'Turn',\n",
       " 'STOPimmigrationNOW',\n",
       " 'YouSuck',\n",
       " 'NoH2b',\n",
       " 'Trudeau',\n",
       " 'Altright',\n",
       " 'DrainTheElites',\n",
       " 'QResearch',\n",
       " 'Veterans',\n",
       " 'BPD',\n",
       " 'secureOURborder',\n",
       " 'homecookedm',\n",
       " 'BoycottTheNFL',\n",
       " 'NationalCastleDoctrine',\n",
       " 'RETWEETHelp',\n",
       " 'NoFinancialSupport',\n",
       " 'ParisAttack',\n",
       " 'no',\n",
       " 'SPAIN',\n",
       " 'PreventableDeaths',\n",
       " 'cocksucker',\n",
       " 'AugustRecess',\n",
       " 'ElectoralCollege',\n",
       " 'endVisaLottery',\n",
       " 'msnbc',\n",
       " 'Terrorism',\n",
       " 'Christian',\n",
       " 'DrainTheDeepStateGREAT',\n",
       " 'NoAMNESTY',\n",
       " 'OperationCookOut',\n",
       " 'comingtoafrica',\n",
       " 'LiberalLogic',\n",
       " 'bitchesstink',\n",
       " 'PeriodProbz',\n",
       " 'FoxNewsThis',\n",
       " 'ImmigrationMoratorium',\n",
       " 'senDiehl',\n",
       " 'karma',\n",
       " 'Gaza',\n",
       " 'italy',\n",
       " 'nuclearban',\n",
       " 'TruthIsPower',\n",
       " 'iwantthewall',\n",
       " 'BuildThatDamnWall',\n",
       " 'MaximumSentencing',\n",
       " 'deportillegalimmigrants',\n",
       " 'notallmen',\n",
       " 'noh4ead',\n",
       " 'VoteThemOut',\n",
       " 'IllegalAlienInvasion',\n",
       " 'FUCKIT',\n",
       " 'sexworkersagainstfeminism',\n",
       " 'IDontCare',\n",
       " 'keepWomenDown',\n",
       " 'shutthehellupwomen',\n",
       " 'EnforceUSLaws8',\n",
       " 'YeastInfestedPussySuits',\n",
       " 'NoSocialism',\n",
       " 'Bui',\n",
       " 'FundTheWholeWall',\n",
       " 'KavanaughConfirmation',\n",
       " 'E',\n",
       " 'ndp',\n",
       " 'WhyIsThisNews',\n",
       " 'Winning',\n",
       " 'KeepAmericansSafeIt',\n",
       " 'FuckBitches',\n",
       " 'WalkAwayMovement',\n",
       " 'BlacksForTrump',\n",
       " 'ENDVOTERFRAUD',\n",
       " 'lilbulli',\n",
       " 'congress',\n",
       " 'TinyLivesAtStake',\n",
       " 'Germany',\n",
       " 'EjectThem',\n",
       " 'TRUMP',\n",
       " 'illegals',\n",
       " 'Britainhttps',\n",
       " 'Budget',\n",
       " 'TaxCuts2',\n",
       " 'HealthNightmare',\n",
       " 'terroristswithin',\n",
       " 'Arbys',\n",
       " 'ElectionDay',\n",
       " 'ANTIOBAMA',\n",
       " 'migrants',\n",
       " 'AllIllegalAliensAreLawbreakers',\n",
       " 'TakeAmericaBack',\n",
       " 'ChooseCruz',\n",
       " 'schizo',\n",
       " 'WWG1WGA',\n",
       " 'crossdresser',\n",
       " 'FamiliesBelongTogether',\n",
       " 'Maga',\n",
       " 'Slovenia',\n",
       " 'DeporThemAll',\n",
       " 'BuildTheDamWall',\n",
       " 'Election2018',\n",
       " 'FastandFurious',\n",
       " 'IllegalAliensYes',\n",
       " 'DeportAllOfThem',\n",
       " 'VoteRED',\n",
       " 'EqualRights',\n",
       " 'LameExcuses',\n",
       " 'ResignRyan',\n",
       " 'fuckwome',\n",
       " 'Russiagate',\n",
       " 'Australia',\n",
       " 'Propaganda',\n",
       " 'BuildThatBridge',\n",
       " 'VoteRepublican2018',\n",
       " 'QAnon',\n",
       " 'SanctuaryCity',\n",
       " 'Build',\n",
       " 'fuckyouRonald',\n",
       " 'NoDACADeport',\n",
       " 'refugeesnotwelcome',\n",
       " '3',\n",
       " 'FreeAssabge',\n",
       " 'ImmigrationLottery',\n",
       " 'DeportIllegals',\n",
       " 'uk',\n",
       " 'WWG1WWGA',\n",
       " 'NoHousing',\n",
       " 'DHS',\n",
       " 'QOTD',\n",
       " 'Sendthemback',\n",
       " 'FUCK',\n",
       " 'ThesePeopleShouldBehanbging',\n",
       " 'Lies',\n",
       " 'ocean',\n",
       " 'DiversityIsOurStrengthhttps',\n",
       " 'DeportIllegalAilens',\n",
       " 'visegrad',\n",
       " 'SendthemBack',\n",
       " 'Nosharia',\n",
       " 'h1bvisa',\n",
       " 'TuesdayThoughtsObama',\n",
       " 'DeportTheMigrants',\n",
       " 'StopCatchAndRelease',\n",
       " 'donaldtrump',\n",
       " 'SkilledTradeEducationNeededNow',\n",
       " 'Spanish',\n",
       " 'Immigraiton',\n",
       " 'BrettKavanaugh',\n",
       " 'INDIA',\n",
       " 'WomenSuck',\n",
       " 'YesAllWomen',\n",
       " 'MAGA2KAG',\n",
       " 'Respect',\n",
       " 'BuildTheWall',\n",
       " 'Jobs',\n",
       " 'Iran',\n",
       " 'BorderSecurityEurope',\n",
       " 'supportICE',\n",
       " 'EndDACAAngel',\n",
       " 'PEDOgate',\n",
       " 'EndCorporateWelfare',\n",
       " 'CorruptDNC',\n",
       " 'Ethiopian',\n",
       " 'Findom',\n",
       " 'ProtectTheChildren',\n",
       " 'nomoremigrants',\n",
       " 'DeportTheCriminals',\n",
       " 'MeritBasedImmigration',\n",
       " 'IndepenceDay',\n",
       " 'BorderSecurity',\n",
       " 'MandatoryEverify',\n",
       " 'TrumpKnew',\n",
       " 'ProtectAmerica',\n",
       " 'EndLotteryVisas',\n",
       " 'LockHisAssUp',\n",
       " 'NationalGuard',\n",
       " 'EnforceImmigrationLaws',\n",
       " 'CoreyBooker',\n",
       " 'racist',\n",
       " 'VoteLeave',\n",
       " 'BlueRipple',\n",
       " 'MMIGRATION',\n",
       " 'weinsteinGate',\n",
       " 'SocialismSUCKS',\n",
       " 'yesallmen',\n",
       " 'DrugCartelsWhat',\n",
       " 'VoteRed',\n",
       " 'SanctuaryCities',\n",
       " '2018Midterms',\n",
       " 'UraniumOne',\n",
       " 'TravelBan',\n",
       " 'TherIsMoreOfThemOutThere',\n",
       " 'Truth',\n",
       " 'RobbingUsBlind',\n",
       " 'NODACA',\n",
       " 'AmericanDreamers',\n",
       " 'CCOT',\n",
       " 'AmericanKids',\n",
       " 'RuleOfLaw',\n",
       " 'AmericaIsFull',\n",
       " 'Italy',\n",
       " 'Refugeescrisis',\n",
       " 'Congress',\n",
       " 'Democrats4Trump',\n",
       " 'UndocumentedIsCriminal',\n",
       " 'Unbelievable',\n",
       " 'FridayFeeIings',\n",
       " 'Meritbased',\n",
       " 'StringerTogether',\n",
       " 'laundryqueen',\n",
       " '43',\n",
       " 'Scum',\n",
       " 'BanCheapSlaveLabor',\n",
       " 'americans',\n",
       " 'BNP',\n",
       " 'defund',\n",
       " 'horses',\n",
       " 'culture',\n",
       " 'Politicians',\n",
       " 'GenerationIdentity',\n",
       " 'Q',\n",
       " 'GOP',\n",
       " 'MuslimBan',\n",
       " 'mmigration',\n",
       " 'LOCKHERUP',\n",
       " 'didyouknow',\n",
       " 'lbc',\n",
       " 'GermanyFirst',\n",
       " 'ImmigrationInvasion',\n",
       " 'SmokingWomenSuck',\n",
       " 'WomanShutUp',\n",
       " 'fuckEU',\n",
       " '1A',\n",
       " 'EndDaca',\n",
       " 'NoSanctuaryCities',\n",
       " 'ReinstateRobynGritz',\n",
       " 'vets',\n",
       " 'German',\n",
       " 'establishment',\n",
       " 'Eritrean',\n",
       " 'Muslim',\n",
       " 'BrexitBetrayal',\n",
       " 'WomensMarch',\n",
       " 'hatesmen',\n",
       " 'Beheard',\n",
       " 'ThursdayThoughts',\n",
       " 'SocialismKills',\n",
       " 'EndIllegalBirthrightCitizenship1',\n",
       " 'Asylum',\n",
       " 'refugeeswelcome',\n",
       " 'humanrights',\n",
       " 'ZeroTolerance',\n",
       " 'OBAMAMESS',\n",
       " 'DemocratsAreTerrorists',\n",
       " 'RacialReplacement',\n",
       " 'OBAMA',\n",
       " 'Vote',\n",
       " 'RealVoterID',\n",
       " 'BanSanctuaryCities',\n",
       " 'ReformTheVA',\n",
       " 'pjnet',\n",
       " 'FavoritePresident',\n",
       " 'TheseBonesWillRiseAgain',\n",
       " 'retablirLeDroitRepublicain',\n",
       " 'onpc',\n",
       " 'DNCVoterFraud',\n",
       " 'Deplorables',\n",
       " 'SorosMoney',\n",
       " 'ICE',\n",
       " 'EndGunContro',\n",
       " 'SENATOR',\n",
       " 'ExposeTerroristTrainingCamps',\n",
       " 'MandateEVerify',\n",
       " 'savesmallbusinesses',\n",
       " 'DemocreatsForTrump',\n",
       " 'bordercontrol',\n",
       " 'SendAllIllegalsHome',\n",
       " 'Visegrad',\n",
       " 'bitch',\n",
       " 'refugeesNOTwelcome',\n",
       " 'FairTax',\n",
       " 'CloseTheBorder',\n",
       " 'Republicans',\n",
       " 'BlackLivesMatter',\n",
       " 'TOcouncil',\n",
       " 'ALWAYSCRYINGBLACK',\n",
       " 'BuildThatWallFollow',\n",
       " 'ThanQ',\n",
       " 'Hypocrisy',\n",
       " 'trumplicans',\n",
       " 'Filth',\n",
       " 'noworkforEWI',\n",
       " 'TrumpTrain',\n",
       " 'metoo',\n",
       " 'MSMSilence',\n",
       " 'EndFamilySeparation',\n",
       " 'D',\n",
       " 'bitcoin',\n",
       " 'IllegalCriminals',\n",
       " 'madmax',\n",
       " 'x1f1fa',\n",
       " 'ice',\n",
       " '2AShallNotBeInfringed',\n",
       " 'NOSexTraffickers',\n",
       " 'ObamaCare',\n",
       " 'TonyPaul',\n",
       " 'v4',\n",
       " 'MensRights',\n",
       " 'ImmigrationIsAWeapon',\n",
       " 'VoterIDLaws',\n",
       " 'NOPediophiles',\n",
       " 'WalkAwayDemocrats2018',\n",
       " 'DemocratsKilledMollieTibbetts',\n",
       " 'TrudeauMustGo',\n",
       " 'football',\n",
       " 'ZEE24KALAK',\n",
       " 'Ontario',\n",
       " 'slums',\n",
       " 'Conservatives',\n",
       " 'WeAreBroke',\n",
       " 'KickOutTheLyers',\n",
       " 'AllLivesMatter',\n",
       " 'QueenSugar',\n",
       " 'BuildT',\n",
       " 'RunRunAway',\n",
       " 'Bitch',\n",
       " 'Cowards',\n",
       " 'EndIllegalImmigration',\n",
       " 'EndImmigration',\n",
       " 'border',\n",
       " 'American',\n",
       " 'SecureTheBorder',\n",
       " 'daisyosauke',\n",
       " 'Patriots',\n",
       " 'StopTheInVasion',\n",
       " 'HateEU',\n",
       " 'NoMoreDeaths',\n",
       " 'BoycottNike',\n",
       " 'rape',\n",
       " 'RedPill',\n",
       " 'DefundSanctuaryCities',\n",
       " 'Perverts',\n",
       " 'Nodaca',\n",
       " 'wine',\n",
       " 'IsGermanyChristian',\n",
       " '100',\n",
       " 'danforthshooter',\n",
       " 'KamalaHarris',\n",
       " 'EndChainMigrationInsist',\n",
       " 'x1f602',\n",
       " 'French',\n",
       " 'Subway',\n",
       " 'WakeUp',\n",
       " 'Christianity',\n",
       " 'multibillion',\n",
       " 'BuildTheWALL',\n",
       " 'WalkAwayMovementGrowing',\n",
       " 'UncheckedProgressivism',\n",
       " 'VeteransFirst',\n",
       " 'MoveForward',\n",
       " 'PrayForPaulGeorge',\n",
       " 'NoCatch',\n",
       " 'gop',\n",
       " 'GetToIt',\n",
       " 'PatriotsUnited',\n",
       " 'WeAllKnewIt',\n",
       " 'BuildItNow',\n",
       " 'AmericanCitizenship',\n",
       " 'PreventableDeath',\n",
       " 'Montana',\n",
       " 'EmmanuelMacron',\n",
       " 'HomeTown',\n",
       " 'MigrationIsJihad',\n",
       " 'trap',\n",
       " 'VoteForTe',\n",
       " 'Buildthewall',\n",
       " 'Immigration',\n",
       " 'stopspreadinglies',\n",
       " 'evilwomen',\n",
       " 'IllegalSentHome',\n",
       " 'satchat',\n",
       " 'DeclassifyTheDocs',\n",
       " 'WALKAWAY',\n",
       " 'EndAllVisas',\n",
       " 'AmericansFirst',\n",
       " 'EconomicIllegalRefugeesMustGoBACK',\n",
       " 'PatriotsFight',\n",
       " 'auspolWe',\n",
       " 'LeaveEU',\n",
       " 'DACA',\n",
       " 'HappyBirthdayAmerica',\n",
       " 'America1st',\n",
       " 'NoMoreImmigrantsHere',\n",
       " 'DrainTheDeepState',\n",
       " 'NOAMNESTY',\n",
       " 'Ttrump',\n",
       " 'bullshit',\n",
       " 'NoChainImmigration',\n",
       " 'Niqab',\n",
       " 'RedNatonRising',\n",
       " 'HumanSmugglers',\n",
       " 'salvini',\n",
       " 'NeverVoteDemocratAgain',\n",
       " 'dems',\n",
       " 'Tr',\n",
       " 'NoDreamers',\n",
       " 'AmericansBeforeIllegals',\n",
       " 'StopFundingSF',\n",
       " 'NoIllegals',\n",
       " 'MandatoryEverity',\n",
       " 'aids2020forall',\n",
       " 'AbolishICE',\n",
       " 'Toronto',\n",
       " 'LateNightThoughts',\n",
       " 'SLUMLOVE',\n",
       " 'WomenBoycottTwitter',\n",
       " 'Deplorable',\n",
       " 'paypig',\n",
       " 'hoe',\n",
       " 'KnowYourPlace',\n",
       " 'stophiringillegals',\n",
       " 'NoSanctuaryStates',\n",
       " 'DeportALLIllegals',\n",
       " 'KeepCriminalsOUT',\n",
       " 'ArrestTheEmployers',\n",
       " 'follo',\n",
       " 'Patriot',\n",
       " 'Trump2020',\n",
       " 'Kavanaugh',\n",
       " 'feminism',\n",
       " 'CAIR',\n",
       " 'FeministLogic',\n",
       " 'ICEHeroes',\n",
       " 'SecuretheBor',\n",
       " 'endDACA',\n",
       " 'FundTheWall',\n",
       " 'FactsMatter',\n",
       " 'ConfirmJudgeKavanaugh',\n",
       " 'StopKanavaugh',\n",
       " 'potus',\n",
       " 'sendthemback',\n",
       " 'OBAMASPYGATE',\n",
       " 'TheFive',\n",
       " 'cyberbullied',\n",
       " 'NOChainMigration',\n",
       " 'Rifle',\n",
       " 'SecureOurBorers',\n",
       " 'Bangladesh',\n",
       " 'illegalimmigration',\n",
       " 'sweden',\n",
       " 'AngelFamlies',\n",
       " 'StopTheGreatReplacement',\n",
       " 'antiwhite',\n",
       " 'FAmiliesBelongTogether',\n",
       " 'Saturdaymorning',\n",
       " 'NOMoreDemocrats',\n",
       " 'ProtectChildrenStop',\n",
       " 'NEWS',\n",
       " 'LiesPeopleTellThemselves',\n",
       " 'EndCatchAndRelease',\n",
       " 'GetBackInTheKitchen',\n",
       " 'Pitbulls',\n",
       " 'femboy',\n",
       " 'DestroyISIS',\n",
       " 'BuildtheWalll',\n",
       " 'Communist',\n",
       " 'ProLife',\n",
       " 'WhereAreAllTheMissingChildren',\n",
       " 'wearethestorm',\n",
       " 'endillegalimmigration',\n",
       " 'BGR',\n",
       " 'TheXFactor',\n",
       " 'R',\n",
       " 'Pentagon',\n",
       " 'FamilySeparation',\n",
       " 'NJ',\n",
       " 'HereToStay',\n",
       " 'business',\n",
       " 'British',\n",
       " 'draft',\n",
       " 'StandUpForYourWesternValues',\n",
       " 'wastehertime2016',\n",
       " 'feminist',\n",
       " 'Feminizer',\n",
       " 'Pompous',\n",
       " 'NotAllMen',\n",
       " 'KateSteinle',\n",
       " 'ActionSpeaksVolumes',\n",
       " 'BITCHBLOOD',\n",
       " 'BREXIT',\n",
       " 'JoeBiden',\n",
       " 'EqualOpportunityDeporter',\n",
       " 'sorrynotsorry',\n",
       " 'Illegals',\n",
       " 'blessed',\n",
       " 'MGTOW',\n",
       " 'idiots',\n",
       " 'noaids2020USA',\n",
       " 'deathpenalty',\n",
       " 'MSM',\n",
       " 'BuildThatWallNow',\n",
       " 'KeepAmericaGreat',\n",
       " 'Salvini',\n",
       " 'MensHealth',\n",
       " 'UnredactedFISAdocuments',\n",
       " 'ProudAmerica',\n",
       " 'VetoBeto',\n",
       " 'moron',\n",
       " 'London',\n",
       " 'BoycottNFL',\n",
       " 'Arizona',\n",
       " 'BuildThatWallU',\n",
       " 'MeToo',\n",
       " 'MamtaBanerjee',\n",
       " 'Greece',\n",
       " 'BuyFromInAndOut',\n",
       " 'Wal',\n",
       " 'Anonymous',\n",
       " 'Immigration2018',\n",
       " 'AngelMoms',\n",
       " 'MoveOn',\n",
       " 'feminized',\n",
       " 'USA',\n",
       " 'immigration2018',\n",
       " 'KeepCountryCaps',\n",
       " 'SargentiniForPrison',\n",
       " '346738',\n",
       " 'walletdrain',\n",
       " 'GTFO',\n",
       " 'BUILDTHEWALL',\n",
       " 'WALL',\n",
       " 'SendHerBack',\n",
       " 'WhoresAtHeart',\n",
       " 'AmericansUnite',\n",
       " 'womendisobey',\n",
       " 'subtweet',\n",
       " 'Preventable',\n",
       " 'SanctuaryStates',\n",
       " 'StandWithTrump',\n",
       " 'MakeSwedenGreatAgain',\n",
       " 'WeThePeople',\n",
       " 'Detain',\n",
       " 'THEBIGDUMP',\n",
       " 'Talkradio',\n",
       " 'TermLimits',\n",
       " 'LockThemUp',\n",
       " 'Elections2018',\n",
       " 'SecureOurBorder',\n",
       " 'x1f1f8',\n",
       " 'Nosurrender',\n",
       " 'RV',\n",
       " 'METOo',\n",
       " 'ICERaids',\n",
       " 'SearchForTruth',\n",
       " 'trans',\n",
       " 'EndChainMigrationHere',\n",
       " 'ClingyAssBitches',\n",
       " 'mollietibbits',\n",
       " 'MeritBased',\n",
       " 'EU',\n",
       " 'FourthofJuly',\n",
       " 'feminists',\n",
       " 'KeepAmericaSafe',\n",
       " 'mosques',\n",
       " 'GodBlessPO',\n",
       " 'BorderPatrol',\n",
       " 'DreamAct',\n",
       " 'Deporthemall',\n",
       " 'stopimmigration',\n",
       " 'LondonBridge',\n",
       " 'Inmigration',\n",
       " 'GreenCardsForDACA',\n",
       " 'BuildThatDamnWallNow',\n",
       " 'TCOT',\n",
       " 'NoDeals',\n",
       " 'snpOUT',\n",
       " 'JerryBrown',\n",
       " 'FeminismIsCancer',\n",
       " 'ControlledMSM',\n",
       " 'DoSomething',\n",
       " 'SpeakUpForTheUSA',\n",
       " 'Tories',\n",
       " 'Nexit',\n",
       " 'ProtectUSA',\n",
       " 'Security',\n",
       " 'feminization',\n",
       " 'ENDDEMOCRATS',\n",
       " 'McCainFuneral',\n",
       " 'WalkAway',\n",
       " 'UN',\n",
       " 'saveh2b',\n",
       " 'AnchorBabies',\n",
       " 'pass',\n",
       " 'MuslimImmigration',\n",
       " 'BuildTheDamnWall',\n",
       " 'SecureOurBord',\n",
       " 'obamalegacy',\n",
       " 'DACAisWACA',\n",
       " 'EnforcingTheLaw',\n",
       " 'WeAlmostDatedBut',\n",
       " 'ExecutiveOrder',\n",
       " 'UsetheArmyCoreOfEngineers',\n",
       " 'FakeFemBot',\n",
       " 'TrumpTrain2020',\n",
       " 'babymomma',\n",
       " 'DefendEurope',\n",
       " 'ObamaSpeech',\n",
       " 'SundayFunday',\n",
       " 'legalvettedimmigrationonly',\n",
       " 'WaterBan',\n",
       " 'AHSCult',\n",
       " 'thencrackdown',\n",
       " 'MyBad',\n",
       " 'PressPauseOnImmigration',\n",
       " 'MS13One',\n",
       " 'foxandfriends',\n",
       " 'BUILDTHATWALL',\n",
       " 'WalkAwayFromDemocrats',\n",
       " 'TuitSiMikeFueraDelGhetto',\n",
       " 'NFLKickoff',\n",
       " 'MakeADifference',\n",
       " 'ISIS',\n",
       " 'RIGHTWING',\n",
       " 'America',\n",
       " 'DoYourJob',\n",
       " 'NoHealthCare',\n",
       " 'ConstitutionalistsOnSCOTUS',\n",
       " 'KAS',\n",
       " 'ObviousChild',\n",
       " 'plasticfree',\n",
       " 'KICKTHEMOUT',\n",
       " 'Paris',\n",
       " 'AndrewGillum',\n",
       " 'Americans',\n",
       " 'NoDACAAmnesty',\n",
       " 'EndSantcuaryCities',\n",
       " 'UEbyebye',\n",
       " 'DeclassifyItAll',\n",
       " 'womenareevil',\n",
       " 'RoundupDACA',\n",
       " 'nodaca',\n",
       " 'JimJordanForSpeaker',\n",
       " 'MaleDominance',\n",
       " 'MIGRANT',\n",
       " 'solarban',\n",
       " 'FullDisclosre',\n",
       " 'endOPT',\n",
       " 'AIDS',\n",
       " 'NoH1b',\n",
       " 'Purge',\n",
       " 'Snowflakes',\n",
       " 'DeportIllegalFamiliesTogether',\n",
       " 'V4',\n",
       " 'SWRM',\n",
       " 'MGA',\n",
       " 'Turnout2018',\n",
       " 'NoAnchorBabies',\n",
       " 'VoteNoOnKavanaugh',\n",
       " 'EnforceUSLaws',\n",
       " 'Socialist',\n",
       " 'EndBirthrightCitizenshipForIllegalAliens',\n",
       " 'BeHeardBeSeen',\n",
       " 'RedWave',\n",
       " 'BlackInkCrewCHI',\n",
       " 'EndIllegalBirthrightCitizenship',\n",
       " 'wetbacks',\n",
       " 'redding',\n",
       " 'Amnesty',\n",
       " 'HR392',\n",
       " 'BuildTHATWall',\n",
       " 'FridayMotivation',\n",
       " 'NRCAssam',\n",
       " 'government',\n",
       " 'KeepAmericansSafe',\n",
       " 'SupportPresidentTrump',\n",
       " 'INDI',\n",
       " '2A',\n",
       " 'PUR',\n",
       " 'JOINTHENRA',\n",
       " 'bigleague',\n",
       " 'endChainMigration',\n",
       " 'Deport',\n",
       " 'InternetBillOfRights',\n",
       " 'SorosRefugees',\n",
       " 'boycottFirstMan',\n",
       " 'redpill',\n",
       " 'Beasts',\n",
       " 'realtalk',\n",
       " 'WorldRefugeeDay',\n",
       " 'LiberalsRuinEverything',\n",
       " 'Paedophiles',\n",
       " 'JimJordan4Speaker',\n",
       " 'EndVoterFraud',\n",
       " 'EuropeanSuicide',\n",
       " 'WakeUpYouWhore',\n",
       " 'StopImmigra',\n",
       " 'WalkAwayFromDemocrats2018',\n",
       " 'AsylumScam',\n",
       " 'LiberalismIsAMentalDisorder',\n",
       " 'Bdaygoals',\n",
       " 'REFUGEESNOTWELCOME',\n",
       " 'ThesePeopleAreEVIL',\n",
       " 'WomenDisobey',\n",
       " 'DefundSantuaryCities',\n",
       " 'whitehouse',\n",
       " 'VoteRed2018',\n",
       " 'NRCForSecureIndia',\n",
       " 'VoterID',\n",
       " 'Prayers',\n",
       " 'NWO',\n",
       " 'SAVEAmerica',\n",
       " 'Democats',\n",
       " 'YesAllWomenBelongInTheKitchen',\n",
       " 'NoDeal',\n",
       " 'Whore',\n",
       " 'london',\n",
       " 'NoSanctuary',\n",
       " 'howdoyoulikeusnow',\n",
       " 'MaleDomination',\n",
       " 'DarkLeft',\n",
       " 'StopMassMigration',\n",
       " 'B',\n",
       " 'DeportthemALL',\n",
       " 'BanBusinessLobby',\n",
       " 'trespassers',\n",
       " 'qanon',\n",
       " 'Baloney',\n",
       " 'LHHHReunion',\n",
       " 'OpioidCrisis',\n",
       " 'HonorYourOath',\n",
       " 'fact',\n",
       " 'Texas',\n",
       " 'Billings',\n",
       " 'WakeUpAmerica',\n",
       " 'MAGA2018If',\n",
       " 'WednesdayWisdom',\n",
       " 'TheRainMakers',\n",
       " 'mickyDhasnoD',\n",
       " 'refugees',\n",
       " 'Rotherham',\n",
       " 'trojanhorse',\n",
       " 'walkaway',\n",
       " 'MoreThanTwoFaces',\n",
       " 'BackTheBlu',\n",
       " 'Kurz',\n",
       " 'HouseGOP',\n",
       " 'cdnpoli',\n",
       " 'patriot',\n",
       " 'BlueLivesMatter',\n",
       " 'ProtectUS',\n",
       " 'nonSense',\n",
       " 'EndBirthrightCitizenship',\n",
       " 'x2764',\n",
       " 'OkayBitch',\n",
       " 'Imtalkingaboutadude',\n",
       " 'AngelFamilies',\n",
       " 'TrumpForever',\n",
       " 'Criminal',\n",
       " 'StopMuslimInvation',\n",
       " 'SocialistPoliticians',\n",
       " 'TheSwamp',\n",
       " 'TraffickStop',\n",
       " 'Globalist',\n",
       " 'women',\n",
       " 'BlackTwitter',\n",
       " 'separatingfamilies',\n",
       " 'DeportIllegalCriminalsNow',\n",
       " 'sheforgottheguacamole',\n",
       " 'noHR392Until',\n",
       " 'NoMore',\n",
       " 'MondayMotivation',\n",
       " 'Merkel',\n",
       " 'WednesdayMotivation',\n",
       " 'TreasonTrials',\n",
       " 'AmericanCommunists',\n",
       " 'ChainDeportation',\n",
       " 'RedWaveRising2018',\n",
       " 'ShutDownInvasionOfIllegalAliens',\n",
       " 'voiceforthevoiceless',\n",
       " 'GoogleIsCIA',\n",
       " 'Men',\n",
       " 'Ethiopia',\n",
       " 'Democrats',\n",
       " 'Brexitgoodnews',\n",
       " 'PoliticsMatters',\n",
       " 'ColoradoDoYourJob',\n",
       " 'SpyGate',\n",
       " 'rapefugee',\n",
       " 'FreeJulian',\n",
       " 'ArcOfHistory',\n",
       " 'borrell',\n",
       " 'abpoli',\n",
       " 'WakeUpAlabama',\n",
       " 'ChewbacaLeg',\n",
       " 'NationalSecurity',\n",
       " 'UK',\n",
       " 'NoMuslimIdeology',\n",
       " 'STOPimmigration',\n",
       " 'crime',\n",
       " 'ilovefood',\n",
       " 'BuildTheWallAmnesty',\n",
       " 'Brexiteer',\n",
       " 'BuildtheWall',\n",
       " 'KeepTexasRed',\n",
       " 'supportEverify',\n",
       " 'ASS',\n",
       " 'illegalAliens',\n",
       " 'SendThemHome',\n",
       " 'GangRape',\n",
       " 'Twictator',\n",
       " 'illegalalien',\n",
       " 'ConfirmKavanaughNOW',\n",
       " 'everify',\n",
       " 'Midterms',\n",
       " 'Utah',\n",
       " 'sendThemBack',\n",
       " 'AmericaFirst',\n",
       " 'SSSS',\n",
       " 'TRUMP2020',\n",
       " 'I',\n",
       " 'Burqa',\n",
       " 'canpoli',\n",
       " 'NOMORERefugees',\n",
       " 'CultureWars',\n",
       " 'NoMs',\n",
       " 'TryHarderMSM',\n",
       " 'YoderMustGo',\n",
       " 'StopTheInflux',\n",
       " 'AfricanCulture',\n",
       " 'BackTheBlue',\n",
       " 'obamalibrary',\n",
       " 'EndDiversity',\n",
       " 'unpluginfowars',\n",
       " 'CommonSenseSolutions',\n",
       " 'LandExpropriationHearings',\n",
       " 'NoToBeto',\n",
       " 'Wall',\n",
       " 'MRA',\n",
       " 'WalkAwayMarch',\n",
       " 'MakeAmericaGreatAgain',\n",
       " 'BuildTheWallhttps',\n",
       " 'ChooseLife',\n",
       " 'MAGa',\n",
       " 'AskTheQ',\n",
       " 'BitchesAreDogs',\n",
       " 'WhiteLivesMatter',\n",
       " 'MerkelInsKnast',\n",
       " 'noasylum',\n",
       " 'Kothen',\n",
       " 'NOIllegals',\n",
       " 'IllegalImmigration',\n",
       " 'abuse',\n",
       " 'EndALLImmigration',\n",
       " 'SneakyKevinYoder',\n",
       " 'SaturdayMorning',\n",
       " 'RedWaveRising',\n",
       " 'IllegalImmigrant',\n",
       " 'Deportthemall',\n",
       " 'batshitcrazy',\n",
       " 'EndDiversityLotteryISIS',\n",
       " 'SancutaryCities',\n",
       " 'NoAmensty',\n",
       " 'MAYORS',\n",
       " 'KeepNewsREAL',\n",
       " 'ILLEGAL',\n",
       " 'Military',\n",
       " 'ShitWomenDontSay',\n",
       " 'RCMP',\n",
       " 'MAGA2020',\n",
       " 'Ihatewome',\n",
       " 'NotABot',\n",
       " 'closeborders',\n",
       " 'usa',\n",
       " 'WomenAgainstFeminism',\n",
       " 'Sandy',\n",
       " 'Correct',\n",
       " 'NoMigrants',\n",
       " 'RollTide',\n",
       " 'EndSanctuaryCities',\n",
       " 'WhenIWentToTheMallI',\n",
       " 'AmericansAreDreamersToo',\n",
       " 'immigrationfraud',\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "corpus_train_en = pd.read_csv('corpus/public_development_en_TaskA/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_en = pd.read_csv('corpus/public_development_en_TaskA/dev_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def lista(text):\n",
    "    lista = []\n",
    "    for w in text:\n",
    "        array = extract_hash_tags(w)\n",
    "        if array !=[]:\n",
    "            for x in array:\n",
    "                lista.append(x)\n",
    "    return lista\n",
    "\n",
    "# sacar hashtag hate text\n",
    "hate_train=corpus_train_en[corpus_train_en['HS'] != 0]\n",
    "text1 = hate_train[hate_train.columns[1]]\n",
    "lista1 = lista(text1)\n",
    "hate_dev=corpus_dev_en[corpus_dev_en['HS'] != 0]\n",
    "text2 = hate_dev[hate_dev.columns[1]]\n",
    "lista2 = lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "c = a | b\n",
    "\n",
    "# sacar hashtag aggressive text\n",
    "aggressive_train=corpus_train_en[corpus_train_en['AG'] != 0]\n",
    "text1 = aggressive_train[aggressive_train.columns[1]]\n",
    "lista1=lista(text1)\n",
    "aggressive_dev=corpus_dev_en[corpus_dev_en['AG'] != 0]\n",
    "text2 = aggressive_dev[aggressive_dev.columns[1]]\n",
    "lista2=lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "d = a | b\n",
    "\n",
    "#sacar todos los hashtag\n",
    "c | d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesando el corpus A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus\n",
    "corpus_train_enA = pd.read_csv('corpus/public_development_en_TaskA/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enA = pd.read_csv('corpus/public_development_en_TaskA/dev_en.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar texto medio limpio para sacar etiquetas POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesarme(file, namefile):    \n",
    "    file[file.columns[1]] = [clean(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "def clean(text):\n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USER\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    #estandarizar\n",
    "    text=re.sub(\"['|´]\", \"’\", text)\n",
    "    \n",
    "    text= replace_all('Dictionary/EN/ENabb.txt', text)      \n",
    "    #text= replace_all('Dictionary/EN/ENslang.txt', text)\n",
    "    text= replace_all('Dictionary/EN/ENcontractions.txt', text)\n",
    "        \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(\"(?:&gt|¤|ð|ÿ|‡|¨|¦|®)\", \" \", text) \n",
    "    text=re.sub(\"&amp\", \" and \", text) \n",
    "    text=re.sub(\"&\", \" and \", text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", text)))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#Guardando el corpus\n",
    "procesarme(corpus_train_enA, \"corpus/public_development_en_TaskA/train_en_cPOSA.tsv\")\n",
    "procesarme(corpus_dev_enA, \"corpus/public_development_en_TaskA/dev_en_cPOSA.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el texto limpio A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardando el corpus ya procesado\n",
    "procesar(corpus_train_enA, \"corpus/public_development_en_TaskA/train_en_cleanA.tsv\")\n",
    "procesar(corpus_dev_enA, \"corpus/public_development_en_TaskA/dev_en_cleanA.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesando el corpus limpio A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus ya procesado super limpio A\n",
    "corpus_train_enA = pd.read_csv('corpus/public_development_en_TaskA/train_en_cleanA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enA = pd.read_csv('corpus/public_development_en_TaskA/dev_en_cleanA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "#corpus_train_es[corpus_train_es.columns[1]]\n",
    "\n",
    "train_idA = corpus_train_enA[corpus_train_enA.columns[0]]\n",
    "X_train_textA = corpus_train_enA[corpus_train_enA.columns[1]].fillna(' ')\n",
    "y_train_hsA = corpus_train_enA[corpus_train_enA.columns[2]]\n",
    "y_train_trA = corpus_train_enA[corpus_train_enA.columns[3]]\n",
    "y_train_agA = corpus_train_enA[corpus_train_enA.columns[4]]\n",
    "\n",
    "test_idA = corpus_dev_enA[corpus_train_enA.columns[0]]\n",
    "X_test_textA = corpus_dev_enA[corpus_dev_enA.columns[1]].fillna(' ')\n",
    "y_test_hsA = corpus_dev_enA[corpus_dev_enA.columns[2]]\n",
    "y_test_trA = corpus_dev_enA[corpus_dev_enA.columns[3]]\n",
    "y_test_agA = corpus_dev_enA[corpus_dev_enA.columns[4]]\n",
    "\n",
    "#leyendo el corpus medio limpio para extracción de otras caracts\n",
    "corpus_train_enCA = pd.read_csv('corpus/public_development_en_TaskA/train_en_cPOSA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enCA = pd.read_csv('corpus/public_development_en_TaskA/dev_en_cPOSA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_A = corpus_train_enCA[corpus_train_enCA.columns[1]].fillna(' ')\n",
    "test_A = corpus_dev_enCA[corpus_dev_enCA.columns[1]].fillna(' ')\n",
    "\n",
    "#leyendo el corpus etiqueta POS\n",
    "corpus_train_enPOSA = pd.read_csv('corpus/public_development_en_TaskA/train_en_cPOSTAGA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enPOSA = pd.read_csv('corpus/public_development_en_TaskA/dev_en_cPOSTAGA.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_posA = corpus_train_enPOSA[corpus_train_enPOSA.columns[1]].fillna(' ')\n",
    "test_posA = corpus_dev_enPOSA[corpus_dev_enPOSA.columns[1]].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(corpus_train_enA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts character n-grams\n",
    "def charNgrams(text, n):\n",
    "    ngrams = []\n",
    "    ngrams = [text[i:i+n]+'_cng' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts word-ngrams, when n=1 is equal to bag of words\n",
    "def wordNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts pos-ngrams, when n=1 is equal to bag of pos\n",
    "def posNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [pos for pos in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'_png' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunto_agresivas = set()\n",
    "words_agresiva = open('Dictionary/agresivas_en.txt', 'r', encoding=\"utf8\")\n",
    "words_agresiva.seek(0)\n",
    "words_agresiva = words_agresiva.read().splitlines()\n",
    "ps = PorterStemmer()\n",
    "for agresiva in words_agresiva:\n",
    "    conjunto_agresivas.add(ps.stem(agresiva))\n",
    "\n",
    "def AggressiveNgrams(text, n):\n",
    "    n_grams = []\n",
    "    tokens = text.split(\" \")\n",
    "    fws = []\n",
    "    for word in tokens:\n",
    "        if ps.stem(word) in conjunto_agresivas:\n",
    "            fws.append(word)\n",
    "    n_grams=[('_'.join(fws[i:i+n])) + \"_awn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams\n",
    "\n",
    "def lexPatterns(text):\n",
    "    patterns=[]\n",
    "    #Extracts patterns\n",
    "    for word in words_agresiva:\n",
    "        w =  re.findall(word, text)\n",
    "        w = ['lex_patt' for p in w]\n",
    "        patterns.extend(w)   \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSkipgrams(text,n):\n",
    "    skipgrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    lista = list(nltk.skipgrams(text, 2, n))\n",
    "    skipgrams = [' '.join(i[0]+' '+ i[1])+'' for i in lista]\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcNgrams(text, n):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    patt=r'\\b(' + ('|'.join(re.escape(key) for key in stop_words)).lstrip('|') + r')\\b'\n",
    "    pattern = re.compile(patt)\n",
    "    text = re.sub(r'[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|]*', \"\",text)\n",
    "    #text = re.sub(r\"[\" + punctuation + \"]*\", \"\", text)\n",
    "    terms = pattern.findall(text)\n",
    "    n_grams=[('_'.join(terms[i:i+n])) + \"_fwn\" for i in range(len(terms)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simbPunctNgrams(text, n):\n",
    "    simb_punt = '.,\\/!$%?¿!¡^&*;:{}=><-_`~()”“\\'\\|'\n",
    "    lis_character = list(text)\n",
    "    fws = []\n",
    "    for c in lis_character:\n",
    "        if c in simb_punt:\n",
    "            fws.append(c)\n",
    "    n_grams=[(' '.join(fws[i:i+n])) + \"_pwn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text,pos,tfs,cn,wn,pn,an,sn,fn,sp):\n",
    "    features = []\n",
    "    \n",
    "    for n in cn:\n",
    "        if n != 0:\n",
    "            features.extend(charNgrams(text,n))\n",
    "    for n in wn:\n",
    "        if n != 0:\n",
    "            features.extend(wordNgrams(text,n))\n",
    "    for n in pn:\n",
    "        if n != 0:\n",
    "            features.extend(posNgrams(pos,n))\n",
    "    for n in an:\n",
    "        if n != 0:\n",
    "            features.extend(AggressiveNgrams(text,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(wordSkipgrams(text,n))\n",
    "    for n in fn:\n",
    "        if n!=0:\n",
    "            features.extend(funcNgrams(tfs,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(simbPunctNgrams(tfs,n))\n",
    "            \n",
    "    features.extend(lexPatterns(text))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all features in a set of 'texts' and return as a string separated with the simbol '&%$'\n",
    "def process_texts(texts,poss,textfs,cn,wn,pn,an,sn,fn,sp):\n",
    "    occurrences=defaultdict(int)\n",
    "    featuresList=[]\n",
    "    featuresDict=Counter()\n",
    "    text_pos= list(zip(texts,poss,textfs))   \n",
    "    for (text,pos,tfs) in text_pos:\n",
    "        features=extract_features(text,pos,tfs,cn,wn,pn,an,sn,fn,sp)\n",
    "        featuresDict.update(features)\n",
    "        featuresList.append('&%$'.join(features))\n",
    "    return featuresList, featuresDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para el archivo de salida A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tsv(testid, predictions):    \n",
    "    d = {'id': testid, 'HS': predictions}\n",
    "    file = pd.DataFrame(data=d)  \n",
    "    file.to_csv('corpus/public_development_en_TaskA/en_a.tsv', sep='\\t', encoding='utf-8', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificador(cn, wn, pn, an, sn, fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textA)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textA)\n",
    "    X_train_vectorized = vect.transform(X_train_textA)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textA, train_posA,train_A, cn, wn, pn, an,sn,fn,sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_hsA))\n",
    "    print('\\t', 'tweets', len(X_train_textA))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_hsA) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_hsA, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_hsA, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_hsA, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "   \n",
    "    modelMnB.fit(X_train_vectorized, y_train_hsA) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_hsA)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_hsA)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_hsA) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_hsA)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textA)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textA, test_posA, test_A,cn, wn, pn, an,sn,fn,sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textA), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    ###### File output ########################\n",
    "    print('Writing output file')\n",
    "    output_tsv(test_idA, predictionsMnB)\n",
    "    print('- File created...', 'answers saved to file:','corpus/public_development_en_TaskA/en_a.tsv')\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_hsA, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_hsA, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_hsA, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_hsA, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_hsA, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_hsA, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_hsA, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_hsA, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_hsA, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_hsA, predictionsReg))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 9000\n",
      "\t tweets 9000\n",
      "\t vocabulary size 710252\n",
      "\t class dictribution Counter({0: 5217, 1: 3783})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.6639887816683885\n",
      "10-Fold Cross-validation Linear SVC 0.6303576095081944\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 1000 unknown texts\n",
      " - Predicting Test\n",
      "Writing output file\n",
      "- File created... answers saved to file: corpus/public_development_en_TaskA/en_a.tsv\n",
      "elapsed time: 10039.027328252792\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.723\n",
      "\t SVC 0.657\n",
      " - F1\n",
      "\t MultinomialNB 0.6911928651059086\n",
      "\t SVC 0.5652724968314322\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "#agregar function words y también ngrams de puntuación\n",
    "\n",
    "clasificador(cnvalues, wnvalues, pnvalues, anvalues,skipgrams,fngrams,spgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
