{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas importadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza en los datos\n",
    "* Cambiar todas las palabras de mayúsculas a minúsculas\n",
    "* Se han eliminado las '@' de @USUARIO con el fin de facilitar el etiquetado morfológico\n",
    "* Quitar los links \n",
    "* Quitar los emojis\n",
    "* Cambiar los slangs, abreviaturas y contracciones en su significado\n",
    "* Se han reemplazado todos los números por el símbolo '0'\n",
    "* Cambiar los hashtag por su palabra agresiva o odiosa\n",
    "* Quitar los signos de puntuación y quitar espacios (tabuladores, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_URL=\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\"\n",
    "\n",
    "def procesar(file, namefile):    \n",
    "    file[file.columns[1]] = [clean_text(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()   \n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USUARIO\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    text= replace_all('Dictionary/SP/SPabb.txt', text)      \n",
    "    text= replace_all('Dictionary/SP/SPslang.txt', text)\n",
    "    text= replace_all('Dictionary/SP/SPcontractions.txt', text)\n",
    "    text= remove_stopwords(text)\n",
    "    text=re.sub(\"\\d+\", \"0\", text)      \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub('(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|])', \" \",text))))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):    \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "    for i in stopwords:\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, \"\", text)\n",
    "    return text\n",
    "\n",
    "def extract_hashtag(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def change_hashtag(text):    \n",
    "    input_file_agresiva = open('Dictionary/agresivas_es.txt', 'r', encoding=\"utf8\")\n",
    "    input_file_agresiva.seek(0)\n",
    "    input_file_agresiva = input_file_agresiva.read().splitlines()\n",
    "    h = extract_hashtag(text)\n",
    "    for cadena in h:\n",
    "        for agresivo in input_file_agresiva:\n",
    "            if cadena.find(agresivo) != -1:\n",
    "                text = text.replace(\"#\"+cadena,agresivo)\n",
    "        text = text.replace(\"#\"+cadena,\"\")\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs                               \n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F1F2\"\n",
    "                               \"\\U0001F1F4\"\n",
    "                               \"\\U0001F620\"\n",
    "                               \"]+\", flags=re.UNICODE)   \n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    return text\n",
    "\n",
    "def replace_all(path, text):\n",
    "    dic = create_dictionary_words(path)    \n",
    "    for i, j in dic.items():\n",
    "        text = re.sub(r\"(^|\\s)%s(\\s|$)\" % i, \" \"+j+\" \", text)\n",
    "        # r\"\\b%s\\b\"% enables replacing by whole word matches only\n",
    "    return text\n",
    "\n",
    "def create_dictionary_words(path):\n",
    "    # create a dictionary of words-to-replace and words-to-replace-with\n",
    "    input_file = open(path, 'r', encoding=\"utf8\")\n",
    "    input_file.seek(0)\n",
    "    input_file = input_file.read().splitlines()\n",
    "    input_array = [w.strip().split('\\t') for w in input_file]\n",
    "    output_dict = dict()\n",
    "    for s in input_array:\n",
    "        output_dict[s[0]]= s[1]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer los hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'17A',\n",
       " '21DARV',\n",
       " '3Ago',\n",
       " 'ASCO',\n",
       " 'AcogidaDigna',\n",
       " 'Afregar',\n",
       " 'AgresiónManterosBcnPdV',\n",
       " 'AlCongresoPorLosJubilados',\n",
       " 'Algeciras',\n",
       " 'AsiaCentral',\n",
       " 'AsíVivimosElRacismo',\n",
       " 'AñoNuevoEnCombate',\n",
       " 'Barcelona',\n",
       " 'BatiArjona',\n",
       " 'BestBoyBand',\n",
       " 'Betis',\n",
       " 'Bienvenidos13',\n",
       " 'C',\n",
       " 'CARMENA',\n",
       " 'CNCO',\n",
       " 'CaerEnTentacion',\n",
       " 'CamilaVallejo',\n",
       " 'Canarias',\n",
       " 'Carmena',\n",
       " 'Catalunya',\n",
       " 'Ceuta',\n",
       " 'Chile',\n",
       " 'ChileParaLosChilenos',\n",
       " 'ChilePrimero',\n",
       " 'CloseBorders',\n",
       " 'Closeborders',\n",
       " 'CostaRica',\n",
       " 'CállateYfriega',\n",
       " 'DeNada',\n",
       " 'DeRegaloTeMereces',\n",
       " 'DebatePorElFuturo',\n",
       " 'DefiendeChile',\n",
       " 'DefiendeEspaña',\n",
       " 'Desmontando',\n",
       " 'DimisionInutilMarlaska',\n",
       " 'DonaldCasado',\n",
       " 'ELECCIONESGENERALESYA',\n",
       " 'ELECCIONESYA',\n",
       " 'ESP',\n",
       " 'EXCLUSIVA',\n",
       " 'ElCascabel01A',\n",
       " 'EleccionesGeneralesYA',\n",
       " 'EnTuJeta',\n",
       " 'EsMuyDeProVida',\n",
       " 'Espana',\n",
       " 'España',\n",
       " 'EspañaLoPrimero',\n",
       " 'Europa',\n",
       " 'ExatlonMx',\n",
       " 'ExperimentoHistorico',\n",
       " 'FelizDomingo',\n",
       " 'FelizLunes',\n",
       " 'FelizSabado',\n",
       " 'Femimoda',\n",
       " 'FronterasSeguras',\n",
       " 'FueraRodolfoNoriega',\n",
       " 'GrandeMarlaska',\n",
       " 'GravedadAnteTodo',\n",
       " 'HechosReales',\n",
       " 'HombreAbusado',\n",
       " 'HoyLeDoyUnfollow',\n",
       " 'Humor',\n",
       " 'Idonotwantmafias',\n",
       " 'Ilegal',\n",
       " 'IllegalImmigration',\n",
       " 'InformateEnHolaChile',\n",
       " 'Inmigración',\n",
       " 'InmigrantesNO',\n",
       " 'JuanaRivasSomosTodas',\n",
       " 'Kirguistán',\n",
       " 'KosemGranFinal',\n",
       " 'L6Nfranco',\n",
       " 'LEL',\n",
       " 'LaAcademia',\n",
       " 'LaCornisa',\n",
       " 'LaSilenciosaCAT',\n",
       " 'Lavapiés',\n",
       " 'MADRILEÑOS',\n",
       " 'MaduroAcabóConVzla',\n",
       " 'MaduroGenocida',\n",
       " 'Manteros',\n",
       " 'Marruecos',\n",
       " 'MedidasYa',\n",
       " 'Mierdadepais',\n",
       " 'MiraQuienBaila',\n",
       " 'MiraculousLadybug',\n",
       " 'MoroMierdas',\n",
       " 'Morocco',\n",
       " 'Moromierdas',\n",
       " 'Motril',\n",
       " 'MásterDeHipocresía',\n",
       " 'Nicaragua',\n",
       " 'No',\n",
       " 'NosTomanPorImbéciles',\n",
       " 'Noticias',\n",
       " 'OTGala8',\n",
       " 'OmarPrieto',\n",
       " 'Openborders',\n",
       " 'POLÍTICA',\n",
       " 'PabloCasado',\n",
       " 'PabloTrump',\n",
       " 'Pap399',\n",
       " 'PartyChilensisFtAñoNuevo',\n",
       " 'PdV06A',\n",
       " 'Pendejos',\n",
       " 'Qatar',\n",
       " 'RacismoEs',\n",
       " 'RevolucionIndestructible',\n",
       " 'Runaways',\n",
       " 'SabiasQue',\n",
       " 'SanchezDimision',\n",
       " 'SanchezDimisión',\n",
       " 'SanchezPresidenteNOelecto',\n",
       " 'SanchezVeteYa',\n",
       " 'SeamosHonestos',\n",
       " 'Sevilla',\n",
       " 'SevillaBetis',\n",
       " 'Señorllevamépronto',\n",
       " 'SinCerebro',\n",
       " 'SinDerechos',\n",
       " 'SinPolla',\n",
       " 'Siria',\n",
       " 'SocialPatriotas2022',\n",
       " 'Spain',\n",
       " 'StopInvasion',\n",
       " 'StopIslam',\n",
       " 'Sub20Fem',\n",
       " 'Sudacas',\n",
       " 'TDF2018',\n",
       " 'TODOCOYOCAN',\n",
       " 'Tanger',\n",
       " 'TeamAlan',\n",
       " 'TetasFree',\n",
       " 'TipicoDeCelosos',\n",
       " 'TodasPutas',\n",
       " 'TomaDosTazas',\n",
       " 'U20WWC',\n",
       " 'UngaUngaArmy',\n",
       " 'Urgente',\n",
       " 'VOX',\n",
       " 'Venezuela',\n",
       " 'WelcomeRefugees',\n",
       " 'YoAborte',\n",
       " 'YouNeverLovedMe',\n",
       " 'YuriIsOverParty',\n",
       " 'africanos',\n",
       " 'autónomos',\n",
       " 'banislam',\n",
       " 'camilavallejo',\n",
       " 'caminodeVenezuela',\n",
       " 'campoderefugiados',\n",
       " 'camposdeautogestión',\n",
       " 'chileprimero',\n",
       " 'deber',\n",
       " 'dedazossánchez',\n",
       " 'deporten',\n",
       " 'elcascabel06a',\n",
       " 'enamorada',\n",
       " 'escuanto',\n",
       " 'españa',\n",
       " 'españaesuna',\n",
       " 'estefinde',\n",
       " 'extranjero',\n",
       " 'extranjeros',\n",
       " 'fapza',\n",
       " 'femimoda',\n",
       " 'feminazi',\n",
       " 'feminazis',\n",
       " 'güero',\n",
       " 'hombres',\n",
       " 'iHeartAwards',\n",
       " 'ilegalmente',\n",
       " 'impromilf',\n",
       " 'inmigracion',\n",
       " 'inmigración',\n",
       " 'inmigrantes',\n",
       " 'l6nveranofranco',\n",
       " 'lanzarote',\n",
       " 'maduroesmuerte',\n",
       " 'maldición',\n",
       " 'manteros',\n",
       " 'migrants',\n",
       " 'migrates',\n",
       " 'minoviaesdelsevillayesunaputamas',\n",
       " 'minovioesgenial',\n",
       " 'morenas',\n",
       " 'moromierdas',\n",
       " 'narcopisos',\n",
       " 'nv',\n",
       " 'okupas',\n",
       " 'parejainfelizSDP',\n",
       " 'perroanimal',\n",
       " 'politivida',\n",
       " 'políticos',\n",
       " 'porn',\n",
       " 'pregunta',\n",
       " 'puralata',\n",
       " 'refugiados',\n",
       " 'resentidos',\n",
       " 'sanidaduniversal',\n",
       " 'sexooral',\n",
       " 'soberanía',\n",
       " 'stopLGTB',\n",
       " 'stopOTAN',\n",
       " 'stopUE',\n",
       " 'stopglobalizacion',\n",
       " 'stopinvasion',\n",
       " 'stopislam',\n",
       " 'todasputas',\n",
       " 'topmanta',\n",
       " 'turismofobia',\n",
       " 'twitter',\n",
       " 'venezuela',\n",
       " 'votojusto',\n",
       " 'yaesmediodia32'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "corpus_train_es = pd.read_csv('corpus/public_development_esTaskA/train_es.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_es = pd.read_csv('corpus/public_development_esTaskA/dev_es.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def lista(text):\n",
    "    lista = []\n",
    "    for w in text:\n",
    "        array = extract_hash_tags(w)\n",
    "        if array !=[]:\n",
    "            for x in array:\n",
    "                lista.append(x)\n",
    "    return lista\n",
    "\n",
    "# sacar hashtag hate text\n",
    "hate_train=corpus_train_es[corpus_train_es['HS'] != 0]\n",
    "text1 = hate_train[hate_train.columns[1]]\n",
    "lista1 = lista(text1)\n",
    "hate_dev=corpus_dev_es[corpus_dev_es['HS'] != 0]\n",
    "text2 = hate_dev[hate_dev.columns[1]]\n",
    "lista2 = lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "c = a | b\n",
    "\n",
    "# sacar hashtag aggressive text\n",
    "aggressive_train=corpus_train_es[corpus_train_es['AG'] != 0]\n",
    "text1 = aggressive_train[aggressive_train.columns[1]]\n",
    "lista1=lista(text1)\n",
    "aggressive_dev=corpus_dev_es[corpus_dev_es['AG'] != 0]\n",
    "text2 = aggressive_dev[aggressive_dev.columns[1]]\n",
    "lista2=lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "d = a | b\n",
    "\n",
    "#sacar todos los hashtag\n",
    "c | d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesando el corpus B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus B\n",
    "corpus_train_esB = pd.read_csv('corpus/public_development_esTaskB/train_es.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esB = pd.read_csv('corpus/public_development_esTaskB/dev_es.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar texto medio limpio para sacar etiquetas POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesarme(file, namefile):    \n",
    "    file[file.columns[1]] = [clean(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "def clean(text):\n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USUARIO\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    text= replace_all('Dictionary/SP/SPabb.txt', text)      \n",
    "    text= replace_all('Dictionary/SP/SPslang.txt', text)\n",
    "    text= replace_all('Dictionary/SP/SPcontractions.txt', text)\n",
    "        \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", text)))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#Guardando el corpus\n",
    "procesarme(corpus_train_esB, \"corpus/public_development_esTaskB/train_es_cPOSB.tsv\")\n",
    "procesarme(corpus_dev_esB, \"corpus/public_development_esTaskB/dev_es_cPOSB.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el texto limpio B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardando el corpus ya procesado B\n",
    "procesar(corpus_train_esB, \"corpus/public_development_esTaskB/train_es_cleanB.tsv\")\n",
    "procesar(corpus_dev_esB, \"corpus/public_development_esTaskB/dev_es_cleanB.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesando el corpus limpio B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus ya procesado B\n",
    "corpus_train_esB = pd.read_csv('corpus/public_development_esTaskB/train_es_cleanB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esB = pd.read_csv('corpus/public_development_esTaskB/dev_es_cleanB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "#corpus_train_esB[corpus_train_esB.columns[1]]\n",
    "\n",
    "train_idB = corpus_train_esB[corpus_train_esB.columns[0]]\n",
    "X_train_textB = corpus_train_esB[corpus_train_esB.columns[1]].fillna(' ')\n",
    "y_train_hsB = corpus_train_esB[corpus_train_esB.columns[2]]\n",
    "y_train_trB = corpus_train_esB[corpus_train_esB.columns[3]]\n",
    "y_train_agB = corpus_train_esB[corpus_train_esB.columns[4]]\n",
    "\n",
    "test_idB = corpus_dev_esB[corpus_train_esB.columns[0]]\n",
    "X_test_textB = corpus_dev_esB[corpus_dev_esB.columns[1]].fillna(' ')\n",
    "y_test_hsB = corpus_dev_esB[corpus_dev_esB.columns[2]]\n",
    "y_test_trB = corpus_dev_esB[corpus_dev_esB.columns[3]]\n",
    "y_test_agB = corpus_dev_esB[corpus_dev_esB.columns[4]]\n",
    "\n",
    "#leyendo el corpus medio limpio para extracción de otras caracts\n",
    "corpus_train_esCB = pd.read_csv('corpus/public_development_esTaskB/train_es_cPOSB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esCB = pd.read_csv('corpus/public_development_esTaskB/dev_es_cPOSB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_B = corpus_train_esCB[corpus_train_esCB.columns[1]].fillna(' ')\n",
    "test_B = corpus_dev_esCB[corpus_dev_esCB.columns[1]].fillna(' ')\n",
    "\n",
    "#leyendo el corpus etiqueta POS\n",
    "corpus_train_esPOSB = pd.read_csv('corpus/public_development_esTaskB/train_es_cPOSTAGB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_esPOSB = pd.read_csv('corpus/public_development_esTaskB/dev_es_cPOSTAGB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_posB = corpus_train_esPOSB[corpus_train_esPOSB.columns[1]].fillna(' ')\n",
    "test_posB = corpus_dev_esPOSB[corpus_dev_esPOSB.columns[1]].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesar los corpus_train que sean hs\n",
    "hate=corpus_train_esB[corpus_train_esB['HS'] != 0]\n",
    "\n",
    "X_train_hs_textB = hate[hate.columns[1]].fillna(' ')\n",
    "y_train_hs_agB = hate[hate.columns[4]]\n",
    "\n",
    "#Procesar los corpus_c que sean hs\n",
    "hate_c=corpus_train_esCB[corpus_train_esCB['HS'] != 0]\n",
    "train_hs_B = hate_c[hate_c.columns[1]].fillna(' ')\n",
    "\n",
    "#Procesar los corpus_pos que sean hs\n",
    "hate_pos=corpus_train_esPOSB[corpus_train_esPOSB['HS'] != 0]\n",
    "train_hs_posB = hate_pos[hate_pos.columns[1]].fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts character n-grams\n",
    "def charNgrams(text, n):\n",
    "    ngrams = []\n",
    "    ngrams = [text[i:i+n]+'_cng' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts word-ngrams, when n=1 is equal to bag of words\n",
    "def wordNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts pos-ngrams, when n=1 is equal to bag of pos\n",
    "def posNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [pos for pos in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'_png' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunto_agresivas = set()\n",
    "words_agresiva = open('Dictionary/agresivas_es.txt', 'r', encoding=\"utf8\")\n",
    "words_agresiva.seek(0)\n",
    "words_agresiva = words_agresiva.read().splitlines()\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "for agresiva in words_agresiva:\n",
    "    conjunto_agresivas.add(stemmer.stem(agresiva))\n",
    "\n",
    "def AggressiveNgrams(text, n):\n",
    "    n_grams = []\n",
    "    tokens = text.split(\" \")\n",
    "    fws = []\n",
    "    for word in tokens:\n",
    "        if stemmer.stem(word) in conjunto_agresivas:\n",
    "            fws.append(word)\n",
    "    n_grams=[('_'.join(fws[i:i+n])) + \"_awn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams\n",
    "\n",
    "def lexPatterns(text):\n",
    "    patterns=[]\n",
    "    #Extracts patterns\n",
    "    for word in words_agresiva:\n",
    "        w = re.findall(word, text)\n",
    "        w = ['lex_patt' for p in w]\n",
    "        patterns.extend(w)   \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morfoPatterns(text):\n",
    "    patterns=[]\n",
    "    #Extracts patterns\n",
    "    \n",
    "    Vb_adj = re.findall(r'vm..000 aq.....',text)\n",
    "    Vb_adj= ['morfo_patt' for p in Vb_adj]\n",
    "    patterns.extend(Vb_adj)\n",
    "    \n",
    "    adj_vb = re.findall(r'aq..... vm..000',text)\n",
    "    adj_vb= ['morfo_patt' for p in adj_vb]\n",
    "    patterns.extend(adj_vb)\n",
    "    \n",
    "    sust_adj = re.findall(r'n.0.000 aq.....',text)\n",
    "    sust_adj= ['morfo_patt' for p in sust_adj]\n",
    "    patterns.extend(sust_adj)\n",
    "    \n",
    "    adj_sust = re.findall(r'aq..... n.0.000',text)\n",
    "    adj_sust= ['morfo_patt' for p in adj_sust]\n",
    "    patterns.extend(adj_sust)\n",
    "    \n",
    "    pron_vb = re.findall(r'pd...... vm..000',text)\n",
    "    pron_vb= ['morfo_patt' for p in pron_vb]\n",
    "    patterns.extend(pron_vb)\n",
    "    \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSkipgrams(text,n):\n",
    "    skipgrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    lista = list(nltk.skipgrams(text, 2, n))\n",
    "    skipgrams = [' '.join(i[0]+' '+ i[1])+'' for i in lista]\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcNgrams(text, n):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"spanish\")\n",
    "    patt=r'\\b(' + ('|'.join(re.escape(key) for key in stop_words)).lstrip('|') + r')\\b'\n",
    "    pattern = re.compile(patt)\n",
    "    text = re.sub(r'[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|]*', \"\",text)\n",
    "    #text = re.sub(r\"[\" + punctuation + \"]*\", \"\", text)\n",
    "    terms = pattern.findall(text)\n",
    "    n_grams=[('_'.join(terms[i:i+n])) + \"_fwn\" for i in range(len(terms)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simbPunctNgrams(text, n):\n",
    "    simb_punt = '.,\\/!$%?¿!¡^&*;:{}=><-_`~()”“\\'\\|'\n",
    "    lis_character = list(text)\n",
    "    fws = []\n",
    "    for c in lis_character:\n",
    "        if c in simb_punt:\n",
    "            fws.append(c)\n",
    "    n_grams=[(' '.join(fws[i:i+n])) + \"_pwn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text,pos,tfs,cn,wn,pn,an,hs_ag,tr,sn,fn,sp):\n",
    "    features = []\n",
    "    for n in cn:\n",
    "        if n != 0:\n",
    "            features.extend(charNgrams(text,n))\n",
    "    for n in wn:\n",
    "        if n != 0:\n",
    "            features.extend(wordNgrams(text,n))\n",
    "    for n in pn:\n",
    "        if n != 0:\n",
    "            features.extend(posNgrams(pos,n))\n",
    "    for n in an:\n",
    "        if n != 0:\n",
    "            features.extend(AggressiveNgrams(text,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(wordSkipgrams(text,n))\n",
    "    for n in fn:\n",
    "        if n!=0:\n",
    "            features.extend(funcNgrams(tfs,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(simbPunctNgrams(tfs,n))\n",
    "    \n",
    "    if hs_ag:\n",
    "        features.extend(lexPatterns(text))\n",
    "    if tr:\n",
    "        features.extend(morfoPatterns(text))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all features in a set of 'texts' and return as a string separated with the simbol '&%$'\n",
    "def process_texts(texts,poss,textfs,cn,wn,pn,an,hs_ag,tr,sn,fn,sp):\n",
    "    occurrences=defaultdict(int)\n",
    "    featuresList=[]\n",
    "    featuresDict=Counter()\n",
    "    text_pos= list(zip(texts,poss,textfs))   \n",
    "    for (text,pos,tfs) in text_pos:\n",
    "        features=extract_features(text,pos,tfs,cn,wn,pn,an,hs_ag,tr,sn,fn,sp)\n",
    "        featuresDict.update(features)\n",
    "        featuresList.append('&%$'.join(features))\n",
    "    return featuresList, featuresDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorHS(cn, wn, pn, an, sn, fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textB, train_posB, train_B,cn,wn,pn,an,True, False,sn,fn,sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_hsB))\n",
    "    print('\\t', 'tweets', len(X_train_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_hsB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_hsB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_hsB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_hsB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_hsB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_hsB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_hsB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_hsB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_hsB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB,test_B,cn,wn,pn,an, True, False,sn,fn,sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_hsB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_hsB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_hsB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_hsB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_hsB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_hsB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_hsB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_hsB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_hsB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_hsB, predictionsReg))\n",
    "    \n",
    "    return predictionsMnB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 4469\n",
      "\t tweets 4469\n",
      "\t vocabulary size 401670\n",
      "\t class dictribution Counter({0: 2631, 1: 1838})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.7542817725587252\n",
      "10-Fold Cross-validation Linear SVC 0.7332681798984042\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 500 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 379.10306000709534\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.794\n",
      "\t SVC 0.764\n",
      " - F1\n",
      "\t MultinomialNB 0.7706013363028953\n",
      "\t SVC 0.7107843137254902\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsHS = clasificadorHS(cnvalues, wnvalues, pnvalues, anvalues,skipgrams, fngrams, spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorTR(cn, wn, pn, an, sn, fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textB, train_posB,train_B,cn,wn,pn,an, False, True,sn, fn, sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_trB))\n",
    "    print('\\t', 'tweets', len(X_train_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_trB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_trB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_trB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_trB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_trB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_trB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_trB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_trB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_trB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB, test_B,cn,wn,pn,an, False, True,sn, fn, sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_trB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_trB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_trB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_trB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_trB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_trB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_trB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_trB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_trB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_trB, predictionsReg))\n",
    "    \n",
    "    return predictionsSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 4469\n",
      "\t tweets 4469\n",
      "\t vocabulary size 400633\n",
      "\t class dictribution Counter({0: 3352, 1: 1117})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.698207039890715\n",
      "10-Fold Cross-validation Linear SVC 0.7481261649587411\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 500 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 179.0982208251953\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.806\n",
      "\t SVC 0.86\n",
      " - F1\n",
      "\t MultinomialNB 0.7015384615384617\n",
      "\t SVC 0.7388059701492536\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[0]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsTR = clasificadorTR(cnvalues, wnvalues, pnvalues, anvalues,skipgrams,fngrams,spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorAG(cn, wn, pn, an,sn,fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textB, train_posB,train_B,cn,wn,pn,an, True, False,sn,fn, sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_agB))\n",
    "    print('\\t', 'tweets', len(X_train_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_agB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_agB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_agB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_agB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_agB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_agB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_agB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB, test_B,cn,wn,pn,an, True, False,sn,fn, sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_agB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_agB, predictionsReg))\n",
    "    \n",
    "    return predictionsMnB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 4469\n",
      "\t tweets 4469\n",
      "\t vocabulary size 401670\n",
      "\t class dictribution Counter({0: 2984, 1: 1485})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.6877082818827899\n",
      "10-Fold Cross-validation Linear SVC 0.6689278295273798\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 500 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 357.78141927719116\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.792\n",
      "\t SVC 0.804\n",
      " - F1\n",
      "\t MultinomialNB 0.712707182320442\n",
      "\t SVC 0.69375\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams = [2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsAG = clasificadorAG(cnvalues, wnvalues, pnvalues, anvalues,skipgrams,fngrams,spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - AG (a partir del corpus train_HS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorAG_hs(cn, wn, pn, an,sn,fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures=process_texts(X_train_hs_textB,train_hs_posB,train_hs_B,cn,wn,pn,an,True,False,sn,fn,sp)\n",
    "    \n",
    "    #vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_hs_agB))\n",
    "    print('\\t', 'tweets', len(X_train_hs_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_hs_agB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_hs_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_hs_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_hs_agB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_hs_agB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_hs_agB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_hs_agB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_hs_agB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_hs_agB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB,test_B,cn,wn,pn,an, True, False,sn,fn, sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_agB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_agB, predictionsReg))\n",
    "    \n",
    "    return predictionsMnB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 1838\n",
      "\t tweets 1838\n",
      "\t vocabulary size 172329\n",
      "\t class dictribution Counter({1: 1485, 0: 353})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.8932615602217251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/.local/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-validation Linear SVC 0.9007236282231977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/.local/lib/python3.5/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 500 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 36.748313665390015\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.356\n",
      "\t SVC 0.434\n",
      " - F1\n",
      "\t MultinomialNB 0.5208333333333334\n",
      "\t SVC 0.5486443381180224\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsAG = clasificadorAG_hs(cnvalues, wnvalues, pnvalues, anvalues,skipgrams,fngrams,spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para el archivo de salida B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tsv(testid, predictionsHS, predictionsTR, predictionsAG):    \n",
    "    d = {'id': testid, 'HS': predictionsHS, 'TR': predictionsTR, 'AG': predictionsAG}\n",
    "    file = pd.DataFrame(data=d)  \n",
    "    file.to_csv('corpus/public_development_esTaskB/es_b.tsv', sep='\\t', encoding='utf-8', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output file\n",
      "- File created... answers saved to file: corpus/public_development_esTaskB/es_b.tsv\n"
     ]
    }
   ],
   "source": [
    "###### File output ########################\n",
    "print('Writing output file')\n",
    "output_tsv(test_idB, predictionsHS, predictionsTR, predictionsAG)\n",
    "print('- File created...', 'answers saved to file:','corpus/public_development_esTaskB/es_b.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
