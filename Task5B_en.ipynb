{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas importadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn import preprocessing\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza en los datos\n",
    "* Cambiar todas las palabras de mayúsculas a minúsculas\n",
    "* Se han eliminado las '@' de @USUARIO con el fin de facilitar el etiquetado morfológico\n",
    "* Quitar los links \n",
    "* Quitar los emojis\n",
    "* Cambiar los slangs, abreviaturas y contracciones en su significado\n",
    "* Se han reemplazado todos los números por el símbolo '0'\n",
    "* Cambiar los hashtag por su palabra agresiva o odiosa\n",
    "* Quitar los signos de puntuación y quitar espacios (tabuladores, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_URL=\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\"\n",
    "\n",
    "def procesar(file, namefile):    \n",
    "    file[file.columns[1]] = [clean_text(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()   \n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USER\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    #estandarizar\n",
    "    text=re.sub(\"['|´]\", \"’\", text)\n",
    "    \n",
    "    #antes de cambiar todas a minúsculas, por si estan en el diccionario de las abb y contractions\n",
    "    text= replace_all('Dictionary/EN/ENabb.txt', text)\n",
    "    #text= replace_all('Dictionary/EN/ENslang.txt', text)\n",
    "    text= replace_all('Dictionary/EN/ENcontractions.txt', text)\n",
    "    \n",
    "    text = text.lower()   \n",
    "    \n",
    "    #luego de cambiar todas a minúsculas, por si estan en el diccionario de las abb y contractions\n",
    "    text= replace_all('Dictionary/EN/ENabb.txt', text)\n",
    "    #text= replace_all('Dictionary/EN/ENslang.txt', text)\n",
    "    text= replace_all('Dictionary/EN/ENcontractions.txt', text)\n",
    "    text= remove_stopwords(text)\n",
    "    \n",
    "    text=re.sub(\"\\d+\", \"0\", text)      \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(\"(?:&gt|¤|ð|ÿ|‡|¨|¦|®)\", \" \", text) \n",
    "    text=re.sub(\"&amp\", \" and \", text) \n",
    "    text=re.sub(\"&\", \" and \", text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", re.sub('(?:[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\|])', \" \",text))))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):    \n",
    "    stopwords=set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    for i in stopwords:\n",
    "        text = re.sub(r\"\\b%s\\b\" % i, \"\", text)\n",
    "    return text\n",
    "\n",
    "def extract_hashtag(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def change_hashtag(text):    \n",
    "    input_file_agresiva = open('Dictionary/agresivas_en.txt', 'r', encoding=\"utf8\")\n",
    "    input_file_agresiva.seek(0)\n",
    "    input_file_agresiva = input_file_agresiva.read().splitlines()\n",
    "    h = extract_hashtag(text)\n",
    "    for cadena in h:\n",
    "        for agresivo in input_file_agresiva:\n",
    "            if cadena.find(agresivo) != -1:\n",
    "                text = text.replace(\"#\"+cadena,agresivo)\n",
    "        text = text.replace(\"#\"+cadena,\"\")\n",
    "    return text\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs                               \n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"\\U00002702-\\U000027B0\"\n",
    "                               \"\\U000024C2-\\U0001F251\"\n",
    "                               \"\\U0001f926-\\U0001f937\"\n",
    "                               \"\\u200d\"\n",
    "                               \"\\u2640-\\u2642\"\n",
    "                               \"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "                               \"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "                               \"\\U0001F600-\\U0001F64F\"\n",
    "                               \"\\U0001F1F2\"\n",
    "                               \"\\U0001F1F4\"\n",
    "                               \"\\U0001F620\"\n",
    "                               \"]+\", flags=re.UNICODE)   \n",
    "    text = emoji_pattern.sub(r'', text) # no emoji\n",
    "    return text\n",
    "\n",
    "def replace_all(path, text):\n",
    "    dic = create_dictionary_words(path)    \n",
    "    for i, j in dic.items():\n",
    "        text = re.sub(r\"(^|\\s)%s(\\s|$)\" % i, \" \"+j+\" \", text)\n",
    "        # r\"\\b%s\\b\"% enables replacing by whole word matches only\n",
    "    return text\n",
    "\n",
    "def create_dictionary_words(path):\n",
    "    # create a dictionary of words-to-replace and words-to-replace-with\n",
    "    input_file = open(path, 'r', encoding=\"utf8\")\n",
    "    input_file.seek(0)\n",
    "    input_file = input_file.read().splitlines()\n",
    "    input_array = [w.strip().split('\\t') for w in input_file]\n",
    "    output_dict = dict()\n",
    "    for s in input_array:\n",
    "        output_dict[s[0]]= s[1]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraer los hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blog',\n",
       " 'AndrewGillum',\n",
       " 'Democrtats',\n",
       " 'senDiehl',\n",
       " 'DefendEurope',\n",
       " 'refugeeswelcome',\n",
       " 'NoVisaLotte',\n",
       " 'values',\n",
       " 'ICERaids',\n",
       " 'maledominance',\n",
       " 'burqa',\n",
       " 'illegalimmigration',\n",
       " 'LockHerUp',\n",
       " 'NODACA',\n",
       " 'meetoo',\n",
       " 'lilbulli',\n",
       " 'TryHarderMSM',\n",
       " 'PlannedParenthood',\n",
       " 'Tories',\n",
       " 'WalkAWay',\n",
       " 'VetoBeto',\n",
       " 'UnredactedFISAdocuments',\n",
       " 'badburri',\n",
       " 'KnowYourPlace',\n",
       " 'remove',\n",
       " 'DeportIllegalFamiliesTogether2',\n",
       " 'carrfire',\n",
       " 'bullshit',\n",
       " 'KeepAmericansSafeNowhere',\n",
       " 'OperationCookOut',\n",
       " 'FavoritePresident',\n",
       " 'VoteThemOut',\n",
       " 'MoreBorderPatrolAgents',\n",
       " 'FastandFurious',\n",
       " 'SendThemBack',\n",
       " 'Gaza',\n",
       " 'DeportAllOfThem',\n",
       " 'KavanaughConfirmation',\n",
       " 'NotSurprised',\n",
       " 'London',\n",
       " 'MAYORS',\n",
       " 'Nosharia',\n",
       " 'Eritrean',\n",
       " 'HonorYourOath',\n",
       " 'lookinthemirror',\n",
       " 'VoteRED',\n",
       " 'ElinErsson',\n",
       " 'Lies',\n",
       " 'WaterBan',\n",
       " 'AmericansBeforeIllegals',\n",
       " 'KamalaHarris',\n",
       " 'Justice',\n",
       " 'CommonSenseSolutions',\n",
       " 'canpoli',\n",
       " 'sorrynotsorry',\n",
       " 'RedTsunami',\n",
       " 'TheICEManCometh',\n",
       " 'DeportAliens',\n",
       " 'I',\n",
       " 'foxandfriends',\n",
       " 'plasticfree',\n",
       " 'PreventableDeath',\n",
       " 'SendthemBack',\n",
       " 'EricHolder',\n",
       " 'MaleDomination',\n",
       " 'boycottvicela',\n",
       " 'FactCheck',\n",
       " 'SundayMotivation',\n",
       " 'PoetsDay',\n",
       " 'RETWEEET',\n",
       " 'MuslimBan',\n",
       " 'MIGRANT',\n",
       " 'HouseGOP',\n",
       " 'Amnesty',\n",
       " 'Czechia',\n",
       " 'blessed',\n",
       " 'MigrantsHow',\n",
       " 'UnitedAgainstSharia',\n",
       " 'TripleWall',\n",
       " 'MSM',\n",
       " 'AAPSU',\n",
       " 'USCitizensOnlyVOTE',\n",
       " 'Hypocrisyhttps',\n",
       " 'Perverts',\n",
       " 'CloseOurBorders',\n",
       " 'supportICE',\n",
       " 'ObamaGate',\n",
       " 'Spain',\n",
       " 'UndocumentedIsCriminal',\n",
       " 'persist',\n",
       " 'ass',\n",
       " 'NationOFislam',\n",
       " 'DestroyISIS',\n",
       " 'كك',\n",
       " 'ImpeachBottoms',\n",
       " 'RedNatonRising',\n",
       " 'secureOURborder',\n",
       " 'Paedophiles',\n",
       " 'IllegalIimmigration',\n",
       " 'AsylumScam',\n",
       " 'shutthehellupwomen',\n",
       " 'keepWomenDown',\n",
       " 'BuildThatW',\n",
       " 'h1bvisa',\n",
       " 'ALWAYSCRYINGBLACK',\n",
       " 'bbcbreakfast',\n",
       " 'INDI',\n",
       " 'Proud',\n",
       " 'WalkAwayMovementGrowing',\n",
       " 'WeAlmostDatedBut',\n",
       " 'CloseTheBorder',\n",
       " 'EnforceUSLaws8',\n",
       " 'DemocratsAreDangerous',\n",
       " 'babymomma',\n",
       " 'DeathPenalty',\n",
       " 'Americafirst',\n",
       " 'WhereAreTheChildren',\n",
       " 'OpenBorders',\n",
       " 'LHHHReunion',\n",
       " 'Fact',\n",
       " 'BanIslam',\n",
       " 'RIGHTWING',\n",
       " 'NoMoreRefugees',\n",
       " 'AssamNRC',\n",
       " 'MorningJoe',\n",
       " 'TuesdayThoughts',\n",
       " 'NoMuslimIdeology',\n",
       " 'YesAllMen',\n",
       " 'SecureTheBorder',\n",
       " 'ArcOfHistory',\n",
       " 'EndSantcuaryCities',\n",
       " 'EndLotteryVisas',\n",
       " 'SpeakEnglish',\n",
       " 'JesseWatters',\n",
       " 'ChildTrafficking',\n",
       " 'responsibleguncontrol',\n",
       " 'RCMP',\n",
       " 'OBAMASPYGATE',\n",
       " 'StringerTogether',\n",
       " 'MAGAVeteran',\n",
       " 'solarban',\n",
       " 'immigrationban',\n",
       " 'ILLEGAL',\n",
       " 'culture',\n",
       " 'evilwomen',\n",
       " 'EndDiversity',\n",
       " 'Walkaway',\n",
       " 'D',\n",
       " 'NoDACADeport',\n",
       " 'Republicans',\n",
       " 'sendThemBack',\n",
       " 'Criminal',\n",
       " 'RT4Trump',\n",
       " 'TheFive',\n",
       " 'WeAreBroke',\n",
       " 'DeportThemAllThis',\n",
       " 'PresidentTrump',\n",
       " 'MensRights',\n",
       " 'BuildThatWa',\n",
       " 'TCOT',\n",
       " 'Labour',\n",
       " 'bitch',\n",
       " 'StopKanavaugh',\n",
       " 'Democracy',\n",
       " 'Conservatives',\n",
       " 'Sendthemback',\n",
       " 'JoeBiden',\n",
       " 'NoChainImmigration',\n",
       " 'NoChainMigration',\n",
       " 'stop_islamization',\n",
       " 'QueenSugar',\n",
       " 'LandExpropriationHearings',\n",
       " 'EndGunContro',\n",
       " 'HalloweenCostumeIdeas',\n",
       " 'LEAVE',\n",
       " 'ice',\n",
       " 'Obama',\n",
       " 'nomorerefugees',\n",
       " 'Anonymous',\n",
       " 'IndecisiveBitches',\n",
       " 'Nosurrender',\n",
       " 'StopOverdoses',\n",
       " 'KeepAmericansSafeThis',\n",
       " 'NoH1b',\n",
       " 'DreamAct',\n",
       " 'feminismiscancer',\n",
       " 'NoH2Bvisa',\n",
       " 'nogozones',\n",
       " 'Overpopulation',\n",
       " 'NationalCastleDoctrine',\n",
       " 'BitchesAreDogs',\n",
       " 'BorderSecurity',\n",
       " 'Resist',\n",
       " 'SargentiniForPrison',\n",
       " 'VoteRepublican2018',\n",
       " 'danforthshooter',\n",
       " 'kag',\n",
       " 'Communist',\n",
       " 'BuildThatBridge',\n",
       " 'freedomofpress',\n",
       " 'refugeesNOTwelcome',\n",
       " 'feminized',\n",
       " 'WhitePride',\n",
       " 'illegalAliens',\n",
       " 'Billings',\n",
       " 'ChewbacaLeg',\n",
       " 'Invaders',\n",
       " 'NJ',\n",
       " 'AIDS',\n",
       " 'DeclassifyItAll',\n",
       " 'NOSexTraffickers',\n",
       " 'DoWhatYouSaid',\n",
       " 'StopOpenBorders',\n",
       " 'Rifle',\n",
       " 'MAGA',\n",
       " 'EconomicIllegalRefugeesMustGoBACK',\n",
       " 'FISA',\n",
       " 'NoAmne',\n",
       " 'OBAMAMESS',\n",
       " 'WhatAboutHerDeportation',\n",
       " 'Refugeescrisis',\n",
       " 'politicalmemes',\n",
       " 'SitDownInTheKitchen',\n",
       " 'abuse',\n",
       " 'follo',\n",
       " 'ConfirmJudgeKavanaugh',\n",
       " 'ImmigrationIsAWeapon',\n",
       " 'WalkAwayMovement',\n",
       " 'AmericaFirstTradePolicy',\n",
       " 'DishonestLeft',\n",
       " 'burtreynolds',\n",
       " 'wtf',\n",
       " 'ProudDeplorable',\n",
       " 'NOPediophiles',\n",
       " 'StopTheGreatReplacement',\n",
       " 'TorontoShooting',\n",
       " 'SexTraffickimg',\n",
       " 'EVerify',\n",
       " 'thereforthevoiceless',\n",
       " 'DrainTheDeepStateGREAT',\n",
       " 'StandwithICE',\n",
       " 'VeteransFirst',\n",
       " 'elite',\n",
       " 'Trudeau',\n",
       " 'VoteRed2018',\n",
       " 'EndGovermentWaste',\n",
       " 'WitchHunt',\n",
       " 'NoSanctuary',\n",
       " 'Illegalimmigrants',\n",
       " 'NeverForget',\n",
       " 'Guwop',\n",
       " 'FactsMatter',\n",
       " 'thencrackdown',\n",
       " 'WALL',\n",
       " 'KAG',\n",
       " 'Meritbased',\n",
       " 'KAG2018',\n",
       " 'noworkforEWI',\n",
       " 'BuildTheWallNow',\n",
       " 'Trumplicans',\n",
       " 'endOPT',\n",
       " 'everify',\n",
       " 'KeepFamiliesTogether',\n",
       " 'StopInvasion',\n",
       " 'NRA',\n",
       " 'massdeportations',\n",
       " 'Orban',\n",
       " 'DEMONS',\n",
       " 'TrumpWho',\n",
       " 'MSMGlobalists',\n",
       " 'BlueRipple',\n",
       " 'NoHousing',\n",
       " 'Turn',\n",
       " 'JesusChrist',\n",
       " 'Compassion',\n",
       " 'whore',\n",
       " 'NoDACADeal',\n",
       " 'ChainMigration',\n",
       " 'EndChainMigration',\n",
       " 'OBAMAgate',\n",
       " 'DRAINTHESWAMP',\n",
       " 'WWG1WGA',\n",
       " 'onpoli',\n",
       " 'CriminalDNC',\n",
       " 'Tr',\n",
       " 'deport',\n",
       " 'SecureOurBorers',\n",
       " 'RollTide',\n",
       " 'ncpol',\n",
       " 'EjectThem',\n",
       " 'migrants',\n",
       " 'BoycottNike',\n",
       " 'BuildThat',\n",
       " 'crossdresser',\n",
       " 'NoAMNESTY',\n",
       " 'msnbc',\n",
       " 'cunt',\n",
       " 'Inmigration',\n",
       " 'AtlantaHitRockBottoms',\n",
       " 'EndAllVisas',\n",
       " 'Berkeley',\n",
       " 'FUCKIT',\n",
       " 'Merkelgeschenke',\n",
       " 'ClingyAssBitches',\n",
       " 'HateEU',\n",
       " 'NoDaca',\n",
       " 'BuildThatDamnWallNow',\n",
       " 'ForThePeople',\n",
       " 'Men',\n",
       " 'TorontoStrong',\n",
       " 'Brexiteers',\n",
       " 'England',\n",
       " 'VoterID',\n",
       " 'SENDTHEMHOME',\n",
       " 'BuildThatWallU',\n",
       " 'ThesePeopleAreEVIL',\n",
       " 'SanctuaryStates',\n",
       " 'BeHeardBeSeen',\n",
       " 'StopImmigra',\n",
       " 'ProudAmerica',\n",
       " 'QAnon',\n",
       " 'BoycottTheNFL',\n",
       " 'R',\n",
       " 'IllegalMigrants',\n",
       " 'RGV',\n",
       " 'AmericansAreDreamersToo',\n",
       " 'KateSteinle',\n",
       " 'Americans',\n",
       " 'RETWEETHelp',\n",
       " 'stress',\n",
       " 'Deplorable',\n",
       " 'Refugeesnotwelcome',\n",
       " 'RedWave',\n",
       " 'Nodaca',\n",
       " 'SorosRefugees',\n",
       " 'YesAllWomenBelongInTheKitchen',\n",
       " 'BarackObama',\n",
       " '911Memorial',\n",
       " 'ThereAreMoreOfThemOutThere',\n",
       " 'MondayMotivation',\n",
       " 'hatesmen',\n",
       " 'noasylum',\n",
       " 'dissoudreLREM',\n",
       " 'PUR',\n",
       " 'MASA',\n",
       " 'TonyPaul',\n",
       " 'NOASYLU',\n",
       " 'TheseBonesWillRiseAgain',\n",
       " 'Sovereignty',\n",
       " 'SecureUSBorders',\n",
       " 'ilovefood',\n",
       " 'RefugeesWelcome',\n",
       " 'SkilledTradeEducationNeededNow',\n",
       " 'FamilySeparation',\n",
       " 'MerkelInsKnast',\n",
       " 'Kurz',\n",
       " 'RoundupDACA',\n",
       " 'ReduireIslamAuSilence',\n",
       " 'WhiteGenocide',\n",
       " 'Islam',\n",
       " 'POS',\n",
       " 'TraffickStop',\n",
       " 'Refugees',\n",
       " 'ProtectChildrenStop',\n",
       " 'openborders',\n",
       " 'AsylumSeekers',\n",
       " 'stopInvasion',\n",
       " 'DACAisWACA',\n",
       " 'cdnpoli',\n",
       " 'NoSANCTUARY',\n",
       " 'VoteDemsOut',\n",
       " 'trumplicans',\n",
       " 'Slovenia',\n",
       " 'RedNationRisinghttps',\n",
       " 'StopIllegalImmigration',\n",
       " 'lbc',\n",
       " 'Illegally',\n",
       " 'AfricanCulture',\n",
       " 'NoSocialism',\n",
       " 'BorderPatrol',\n",
       " 'aids2018',\n",
       " 'ProgressivePolicies',\n",
       " 'UnderminesOurDemocracy',\n",
       " 'CAIR',\n",
       " 'OctoberRally',\n",
       " 'armsesh420',\n",
       " 'TheSwamp',\n",
       " 'ICE',\n",
       " 'CivilWar',\n",
       " 'IndepenceDay',\n",
       " 'BUILDTHEWALL',\n",
       " 'Corruption',\n",
       " 'topoli',\n",
       " 'Globalist',\n",
       " 'WWG1WWGA',\n",
       " 'forza',\n",
       " 'StopTheInVasion',\n",
       " 'BuildThatDamnWall',\n",
       " 'pussy',\n",
       " 'TrumpRally',\n",
       " 'ResignRyan',\n",
       " 'NoDACAAmnesty',\n",
       " 'ProLife',\n",
       " 'MigrationIsJihad',\n",
       " 'slums',\n",
       " 'terroristswithin',\n",
       " 'deportALLaliens',\n",
       " 'BacktheBlue',\n",
       " 'TherIsMoreOfThemOutThere',\n",
       " 'IllegalCriminals',\n",
       " 'sharialaws',\n",
       " 'Democats',\n",
       " 'GodBlessOurPOTUS',\n",
       " 'EndCatchandRelease',\n",
       " 'TravelBan',\n",
       " 'x1f1f8',\n",
       " '2018Midterms',\n",
       " 'UK',\n",
       " 'moron',\n",
       " 'ReleaseFisaReports',\n",
       " 'PreventableDeaths',\n",
       " 'HoneyMoonPoon',\n",
       " 'illegals',\n",
       " 'FuckBitches',\n",
       " 'PressPauseOnImmigration',\n",
       " 'Trumpbots',\n",
       " 'fuckbitches',\n",
       " 'WOMENAREDEMONS',\n",
       " 'Propaganda',\n",
       " 'whitehouse',\n",
       " 'boycottFirstMan',\n",
       " 'TOcouncil',\n",
       " 'Arizona',\n",
       " 'metoo',\n",
       " 'PoliticalIslam',\n",
       " 'EndDACA',\n",
       " 'NotABot',\n",
       " 'DoSomething',\n",
       " 'feminist',\n",
       " 'ISIS',\n",
       " 'dems',\n",
       " 'futanari',\n",
       " 'Rapists',\n",
       " 'GetOut',\n",
       " 'BorderWall',\n",
       " 'AnimalBehavior',\n",
       " 'GaysForTrump',\n",
       " 'UKIP',\n",
       " 'Findom',\n",
       " 'StandUpnBeHeard',\n",
       " 'ndp',\n",
       " 'Slut',\n",
       " '2AShallNotBeInfringed',\n",
       " 'media',\n",
       " 'TrumpArmy',\n",
       " 'AngelFamlies',\n",
       " 'BuildTh',\n",
       " 'SedThemHome',\n",
       " 'Scum',\n",
       " 'refugee',\n",
       " 'RESIST',\n",
       " 'NoAmnesty',\n",
       " 'beer',\n",
       " 'stopspreadinglies',\n",
       " 'MaximumSentencing',\n",
       " 'Correct',\n",
       " 'ImmigrationLottery',\n",
       " 'TwoFaced',\n",
       " 'Preventable',\n",
       " 'NOMORERefugees',\n",
       " 'LeaveEU',\n",
       " 'BuildTheDamnWall',\n",
       " 'Wal',\n",
       " 'endDACA',\n",
       " 'NoDeals',\n",
       " 'GoDie',\n",
       " 'Myanmar',\n",
       " 'FridayFeeling',\n",
       " 'feminization',\n",
       " 'NODAC',\n",
       " 'EndHealthcareCartel',\n",
       " 'WomenAgainstFeminism',\n",
       " 'Hungary',\n",
       " 'TheStorm',\n",
       " 'NoLadyLike',\n",
       " 'Bangladesh',\n",
       " 'MoveOn',\n",
       " 'WalkAwayFromDemocrats',\n",
       " 'PatriotsUnite',\n",
       " 'SecureBorders',\n",
       " 'EqualRights',\n",
       " 'StopWhiteGenocide',\n",
       " 'Constitution',\n",
       " 'Brexiteer',\n",
       " 'BrexitBetrayal',\n",
       " 'FUCK',\n",
       " 'DAC',\n",
       " 'WoodyAllen',\n",
       " '43',\n",
       " 'BNP',\n",
       " 'NOMoreDemocrats',\n",
       " 'French',\n",
       " 'Australia',\n",
       " 'AnnCoulter',\n",
       " 'fuckyouRonald',\n",
       " 'AmericansFirst',\n",
       " 'Military',\n",
       " 'trans',\n",
       " 'weedthoughts',\n",
       " 'DraintheSwa',\n",
       " 'WhyIsThisNews',\n",
       " 'DeportIllegals',\n",
       " 'DeporThemAll',\n",
       " 'Filth',\n",
       " 'RejectImmigrantApplicants',\n",
       " 'feminism',\n",
       " 'ENDVOTERFRAUD',\n",
       " 'Soros',\n",
       " 'USA',\n",
       " 'FoxNews',\n",
       " 'ConstitutionalistsOnSCOTUS',\n",
       " 'womendisobey',\n",
       " 'NOcitizenship',\n",
       " 'Stoptheinvasion',\n",
       " 'madmax',\n",
       " 'Texas',\n",
       " 'wetbacks',\n",
       " 'Rohingya',\n",
       " 'Somalia',\n",
       " 'STOPimmigration',\n",
       " 'NationalSecurity',\n",
       " 'ProudConservative',\n",
       " 'IngrahamAngle',\n",
       " 'AHSCult',\n",
       " 'TheXFactor',\n",
       " 'Bitch',\n",
       " 'Indonesia',\n",
       " 'Spanish',\n",
       " 'SocialismSUCKS',\n",
       " 'PreventableCrime',\n",
       " 'reality',\n",
       " 'IllegalImmigrants',\n",
       " 'LawAndOrder',\n",
       " 'NO',\n",
       " 'IllegalImmigration',\n",
       " 'Turnout2018',\n",
       " 'bbcqt',\n",
       " 'HereToStay',\n",
       " 'WorldRefugeeDay',\n",
       " 'findom',\n",
       " 'triggered',\n",
       " 'NRCAssam',\n",
       " 'BanSanctuaryCities',\n",
       " 'unpluginfowars',\n",
       " 'FullDisclosre',\n",
       " 'NoAmnestyEver',\n",
       " 'Feminizer',\n",
       " 'MeritBasedImmigration',\n",
       " 'SecureOurBord',\n",
       " 'DogRighthttps',\n",
       " 'BanBusinessLobby',\n",
       " 'ChainDeportation',\n",
       " 'BulldTheWall',\n",
       " 'DNCVoterFraud',\n",
       " 'TuckerCarlsonTonight',\n",
       " 'STOPimmigrationNOW',\n",
       " 'FightTheLeft',\n",
       " 'WomensMarch',\n",
       " 'ColoradoDoYourJob',\n",
       " 'FourthofJuly',\n",
       " 'Arbys',\n",
       " 'endVisaLottery',\n",
       " 'business',\n",
       " 'Ttrump',\n",
       " 'KickOutTheLyers',\n",
       " 'IllegalAlienInvasion',\n",
       " 'China',\n",
       " 'Altright',\n",
       " 'IndiaForIndians',\n",
       " 'racist',\n",
       " 'mtf',\n",
       " 'NoAmensty',\n",
       " 'Truth',\n",
       " 'JalalabadAttack',\n",
       " 'ObviousChild',\n",
       " 'BuildT',\n",
       " 'politics',\n",
       " 'KeepAmericansSafeStop',\n",
       " 'TaxCutReformBill',\n",
       " 'WomanShutUp',\n",
       " 'Purge',\n",
       " 'Twictator',\n",
       " 'CrookedHillary',\n",
       " 'RunRunAway',\n",
       " 'auspolWe',\n",
       " 'didyouknow',\n",
       " 'Merkel',\n",
       " 'FeminismIsCancer',\n",
       " 'menS',\n",
       " 'ApologyTourIsOver',\n",
       " 'GenerationIdentity',\n",
       " 'SaturdayMotivation',\n",
       " 'EndChainMigrationInsist',\n",
       " 'AngelMoms',\n",
       " 'Pathetic',\n",
       " 'AMERICAFIRST',\n",
       " 'SmokingWomenSuck',\n",
       " 'DeportALLIllegals',\n",
       " 'sarbanandasonowal',\n",
       " 'Patriots',\n",
       " 'AgainstAllEnemies',\n",
       " 'ref',\n",
       " 'DrawAndQuarter',\n",
       " 'government',\n",
       " 'GetOutTheVote',\n",
       " 'IllegalAliens',\n",
       " 'GreatAwakening',\n",
       " 'Greece',\n",
       " 'BorderSecurityEurope',\n",
       " 'TomBrady',\n",
       " 'BanCheapSlaveLabor',\n",
       " 'womenareevil',\n",
       " 'JusticeForMollieTibbetts',\n",
       " 'RV',\n",
       " 'SanctuaryCities',\n",
       " 'WednesdayWisdom',\n",
       " 'Trumptrain',\n",
       " 'WhereAreAllTheMissingChildren',\n",
       " 'FundTheWall',\n",
       " 'HealthNightmare',\n",
       " 'MakePolioGreatAgain',\n",
       " 'ARABIANSEA',\n",
       " 'ILOVEICE',\n",
       " 'african',\n",
       " 'AmericansHaveDreamsToo',\n",
       " 'Jalalabad',\n",
       " 'SaudiArabia',\n",
       " 'NoCommunists',\n",
       " 'SundayMorning',\n",
       " 'RETWEET',\n",
       " 'Ontario',\n",
       " 'BuildThatWallNow',\n",
       " 'German',\n",
       " 'IllegalAliensYes',\n",
       " 'AngelFamilies',\n",
       " 'BackTheBlue',\n",
       " 'noamnesty',\n",
       " 'mosques',\n",
       " 'PMLN',\n",
       " 'NoFinancialSupport',\n",
       " 'nohr392',\n",
       " 'AllIllegalAliensAreLawbreakers',\n",
       " 'MakeAmericaSafeAgain',\n",
       " 'EndCatchAndRelease',\n",
       " 'KeepCriminalsOUT',\n",
       " 'NOAMNESTY',\n",
       " 'TakeAmericaBack',\n",
       " 'x1f602',\n",
       " 'CoryBooker',\n",
       " 'SLUMLOVE',\n",
       " 'UN',\n",
       " 'American',\n",
       " 'bookerfordogcatcher2020',\n",
       " 'Dobbs',\n",
       " 'Migrants',\n",
       " 'voiceforthevoiceless',\n",
       " 'ilovemymom',\n",
       " 'ChildRape',\n",
       " 'NoSanctuaryCities',\n",
       " 'NoH1Bvisa',\n",
       " 'Christianity',\n",
       " 'comingtoafrica',\n",
       " 'Democraps',\n",
       " 'HomeTown',\n",
       " 'v4',\n",
       " 'DHS',\n",
       " 'Nexit',\n",
       " 'TinyLivesAtStake',\n",
       " 'MigrantCrisis',\n",
       " 'truth',\n",
       " 'THEBIGDUMP',\n",
       " 'Budget',\n",
       " 'angelfamilies',\n",
       " 'Paris',\n",
       " 'homecookedm',\n",
       " 'ReligionofSex',\n",
       " 'BrexitBorder',\n",
       " 'StopTheInflux',\n",
       " '346738',\n",
       " 'womenaredumb',\n",
       " 'AttentionWhores',\n",
       " 'BUILDTHATWALL',\n",
       " 'Politicians',\n",
       " 'Israel',\n",
       " 'AmericaisforAmericans',\n",
       " 'failed',\n",
       " 'MandatoryEVerify',\n",
       " 'Trumpville',\n",
       " 'PedoGate',\n",
       " 'london',\n",
       " 'Karma',\n",
       " 'DrainEurope',\n",
       " 'dingotwitter',\n",
       " 'batshitcrazy',\n",
       " 'EndBirthrightCitizenship',\n",
       " 'NoDreamers',\n",
       " 'NeverVoteDemocratAgain',\n",
       " 'ShitWomenDontSay',\n",
       " 'Immigration',\n",
       " 'DeportTheMigrants',\n",
       " 'Stopimmigration',\n",
       " 'ProtectTheChildren',\n",
       " 'nomoremigrants',\n",
       " 'SorosMoney',\n",
       " 'AbortionGenocide',\n",
       " 'METOo',\n",
       " 'WelfareJockeys',\n",
       " 'ControlledMSM',\n",
       " '3',\n",
       " 'WhereDoISendTheCheck',\n",
       " 'MS13One',\n",
       " 'ICEHeroes',\n",
       " 'EndDACAAngel',\n",
       " 'Democrat',\n",
       " 'MSMSilence',\n",
       " 'closetheborders',\n",
       " 'DeportThemAll',\n",
       " 'Goldmenschen',\n",
       " 'totallyrealnews',\n",
       " 'italy',\n",
       " 'Socialist',\n",
       " 'dildo',\n",
       " 'deportillegalimmigrants',\n",
       " 'JimJordan4Speaker',\n",
       " 'TrumpForever',\n",
       " 'MandatoryEverify',\n",
       " 'NoDEMS',\n",
       " 'WomenDisobey',\n",
       " 'Kothen',\n",
       " 'Rotherham',\n",
       " 'WomenSuckk',\n",
       " 'Burundi',\n",
       " 'AmericanCommunists',\n",
       " 'BuildThatWallSurviving',\n",
       " 'deathpenalty',\n",
       " 'Appropriations',\n",
       " 'SENDTHEMBACK',\n",
       " 'NewChainMigration',\n",
       " 'POTUS',\n",
       " 'SOCIALIST',\n",
       " 'ThesePeopleAreEvil',\n",
       " 'LameExcuses',\n",
       " 'EndCatchAndReleash',\n",
       " 'HonorTheFlag',\n",
       " 'UniParty',\n",
       " 'wine',\n",
       " 'SendThemHome',\n",
       " 'salvini',\n",
       " 'CCOT',\n",
       " 'RealVoterID',\n",
       " 'PatriotsUnited',\n",
       " 'KeepAmericansSafe',\n",
       " 'visegrad',\n",
       " 'Cowards',\n",
       " 'makemeasandwich',\n",
       " 'MyBad',\n",
       " 'stoptheinvasion',\n",
       " 'WALKAWAY',\n",
       " 'HumanTrafficking',\n",
       " 'aids2020forall',\n",
       " 'blowjob',\n",
       " 'BuildTheWallhttps',\n",
       " 'KeepAmericansSafeIt',\n",
       " 'CorruptDNC',\n",
       " 'bitcoin',\n",
       " 'ShutDownTerroristTrainingCamps',\n",
       " 'immigration',\n",
       " 'KAS',\n",
       " 'RedNation',\n",
       " 'NRCForSecureIndia',\n",
       " 'EnactEVerify',\n",
       " 'EndIllegalBirthrightCitizenship1',\n",
       " 'KeepAmericaGreat',\n",
       " 'Deport',\n",
       " 'ExposeTerroristTrainingCamps',\n",
       " 'purge',\n",
       " 'NOIllegals',\n",
       " 'StopImmigration',\n",
       " 'Saturdaymorning',\n",
       " 'sendthemback',\n",
       " 'KavanaghHearing',\n",
       " 'trespassers',\n",
       " 'RedPill',\n",
       " 'howdoyoulikeusnow',\n",
       " 'ihatefemales',\n",
       " 'BackTheBlu',\n",
       " 'MGA',\n",
       " 'MAGA2KAG',\n",
       " 'BuildtheWalll',\n",
       " 'BGR',\n",
       " 'GangRape',\n",
       " 'winning',\n",
       " 'JimJordanForSpeaker',\n",
       " 'StopAsylumScam',\n",
       " 'VoteRed',\n",
       " 'AmericansUnite',\n",
       " 'كيك',\n",
       " 'FridayMotivation',\n",
       " 'gop',\n",
       " 'thewall',\n",
       " 'DeportIllegalAliens',\n",
       " 'cocksucker',\n",
       " 'AlertTheDayCareStaff',\n",
       " 'Democrats4Trump',\n",
       " 'SomebodyToTris',\n",
       " 'SendThemAllBack',\n",
       " 'immigrationfraud',\n",
       " 'walletdrain',\n",
       " 'ReinstateRobynGritz',\n",
       " 'MaleDominance',\n",
       " 'FairTax',\n",
       " 'EndFamilySeparation',\n",
       " 'trojanhorse',\n",
       " 'EndChainMigrationHere',\n",
       " 'FirstofAll',\n",
       " 'Montana',\n",
       " 'BrettKavanaugh',\n",
       " 'immigration2018',\n",
       " 'VoteOutTheDems',\n",
       " 'TrudeauMustGo',\n",
       " 'uk',\n",
       " 'noaids2020USA',\n",
       " 'mmigration',\n",
       " 'x1f621',\n",
       " 'TRUMP',\n",
       " 'DeportIllegalFamiliesTogether',\n",
       " 'nonSense',\n",
       " 'Ihatewome',\n",
       " 'EnforceUSLaws',\n",
       " 'SaveTheChildren',\n",
       " 'BlackLivesMatter',\n",
       " 'DeportthemALL',\n",
       " 'MandatoryEverity',\n",
       " 'NoH2b',\n",
       " 'saveh2b',\n",
       " 'SaveTheWorldUnderTheOneTrueGod',\n",
       " 'BoycottNFLSponsors',\n",
       " 'DrainTheDeepState',\n",
       " 'tcot',\n",
       " 'nuclearban',\n",
       " 'mickyDhasnoD',\n",
       " 'YouHaveToGoBack',\n",
       " 'OkayBitch',\n",
       " 'Buildthatwall',\n",
       " 'BlackTwitter',\n",
       " 'StupidWomen',\n",
       " 'Sodyoueu',\n",
       " 'WeAllKnewIt',\n",
       " '2Amendment',\n",
       " 'marr',\n",
       " 'ANTIOBAMA',\n",
       " 'OnGod',\n",
       " 'ICEAgents',\n",
       " 'YeastInfestedPussySuits',\n",
       " 'donaldtrump',\n",
       " 'SpyGate',\n",
       " 'SupportPresidentTrump',\n",
       " 'Hypocrisy',\n",
       " 'Prayers',\n",
       " 'MakeAmericaGreatAgain',\n",
       " 'noh4ead',\n",
       " 'NoDeal',\n",
       " 'pjnet',\n",
       " 'CommonSense',\n",
       " 'Criminals',\n",
       " 'FamiliesBelongTogetherMarch',\n",
       " 'complicediSalvini',\n",
       " 'ChooseCruz',\n",
       " 'EndDaca',\n",
       " 'Jesus',\n",
       " 'schizo',\n",
       " 'Snowflakes',\n",
       " 'Muslims',\n",
       " 'westernworld',\n",
       " 'Talkradio',\n",
       " 'TrumpOCaptainMyCaptain',\n",
       " 'Remove',\n",
       " 'CultureWars',\n",
       " 'FakeFugees',\n",
       " 'SearchForTruth',\n",
       " 'NoMoreDeaths',\n",
       " 'EndBirthrightCitizenshipForIllegalAliens',\n",
       " 'FreeJulian',\n",
       " 'Trash',\n",
       " 'SendHerBack',\n",
       " 'WhoresAtHeart',\n",
       " 'Soverignty',\n",
       " 'JUDGES',\n",
       " 'ConfirmKavanaughNOW',\n",
       " 'spain',\n",
       " 'TermLimits',\n",
       " 'buildthatwall',\n",
       " 'ProtectUSA',\n",
       " 'TeenWolf',\n",
       " 'MakeItEvenHigher',\n",
       " 'LiberalsRuinEverything',\n",
       " 'r4today',\n",
       " 'EndImmigration',\n",
       " 'NOChainMigration',\n",
       " 'Beheard',\n",
       " 'horses',\n",
       " 'EnoughIsEnough',\n",
       " 'FundTheWholeWall',\n",
       " 'AllLivesMatter',\n",
       " 'x2764',\n",
       " 'STOPTHEMNOW',\n",
       " 'college',\n",
       " 'Congress',\n",
       " 'SocialistPoliticians',\n",
       " 'Terrorism',\n",
       " 'WompWomp',\n",
       " 'satchat',\n",
       " 'ThesePeopleShouldBehanbging',\n",
       " 'FAKENEWS',\n",
       " 'ClosetheLoopholes',\n",
       " 'americans',\n",
       " 'NoToBeto',\n",
       " 'BuildItNow',\n",
       " 'draft',\n",
       " 'LeaveDNC',\n",
       " 'ZEE24KALAK',\n",
       " 'MGTOW',\n",
       " 'GRENZENDICHT',\n",
       " 'NoPleaBargain',\n",
       " 'DeportIllegalAilens',\n",
       " 'OrganizedCrime',\n",
       " 'respectvictoria',\n",
       " 'WalkAway',\n",
       " 'traps',\n",
       " 'menstruafacts',\n",
       " 'Bu',\n",
       " 'ExtremVetting',\n",
       " 'retweetthisone',\n",
       " 'BlacksGoneRed',\n",
       " 'RepealObamacare',\n",
       " 'katesteinle',\n",
       " 'IslamicMigrants',\n",
       " 'SendThe',\n",
       " 'oceans',\n",
       " 'SecretSociety',\n",
       " 'Netherlands',\n",
       " 'degagerMacron',\n",
       " 'TEDCRUZ',\n",
       " 'NEWS',\n",
       " 'Mexico',\n",
       " 'Subway',\n",
       " 'EndALLImmigration',\n",
       " 'NoCatch',\n",
       " 'muslimmigrants',\n",
       " 'DESenate',\n",
       " 'walkaway',\n",
       " 'StopTheI',\n",
       " 'MandateEVerify',\n",
       " 'Whore',\n",
       " 'Feminist',\n",
       " 'closeborders',\n",
       " 'NeverForgetKick',\n",
       " '100',\n",
       " 'Brexitgoodnews',\n",
       " 'CorruptDOJFBI',\n",
       " 'noHR392Until',\n",
       " 'SanctuaryCity',\n",
       " 'nationalism',\n",
       " 'ChooseLife',\n",
       " 'MoreOfThisPlease',\n",
       " 'SneakyKevinYoder',\n",
       " 'LockHisAssUp',\n",
       " 'AbolishICE',\n",
       " 'sweden',\n",
       " 'ReleaseTheUnredactedFisaReports',\n",
       " 'GermanyFirst',\n",
       " 'obamalibrary',\n",
       " 'KimKardasian',\n",
       " 'supportEverify',\n",
       " 'MagaOneVoice',\n",
       " 'ActionSpeaksVolumes',\n",
       " 'DACA',\n",
       " 'FamiliesBelongTogether',\n",
       " 'SeeSomethingSaySomething',\n",
       " 'Danforth',\n",
       " 'vets',\n",
       " 'ShutDownInvasionOfIllegalAliens',\n",
       " 'TRUMPHASMYVOTEIN2020',\n",
       " 'FordNation',\n",
       " 'MondayMorning',\n",
       " 'HappyNewYear',\n",
       " 'EliminateRegulations',\n",
       " 'FollowYourOath',\n",
       " 'DeportByCatapulting',\n",
       " 'bitchesstink',\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "corpus_train_en = pd.read_csv('corpus/public_development_en_TaskA/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_en = pd.read_csv('corpus/public_development_en_TaskA/dev_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "def extract_hash_tags(s):\n",
    "    hs = re.findall(r\"#(\\w+)\", s)\n",
    "    return hs\n",
    "\n",
    "def lista(text):\n",
    "    lista = []\n",
    "    for w in text:\n",
    "        array = extract_hash_tags(w)\n",
    "        if array !=[]:\n",
    "            for x in array:\n",
    "                lista.append(x)\n",
    "    return lista\n",
    "\n",
    "# sacar hashtag hate text\n",
    "hate_train=corpus_train_en[corpus_train_en['HS'] != 0]\n",
    "text1 = hate_train[hate_train.columns[1]]\n",
    "lista1 = lista(text1)\n",
    "hate_dev=corpus_dev_en[corpus_dev_en['HS'] != 0]\n",
    "text2 = hate_dev[hate_dev.columns[1]]\n",
    "lista2 = lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "c = a | b\n",
    "\n",
    "# sacar hashtag aggressive text\n",
    "aggressive_train=corpus_train_en[corpus_train_en['AG'] != 0]\n",
    "text1 = aggressive_train[aggressive_train.columns[1]]\n",
    "lista1=lista(text1)\n",
    "aggressive_dev=corpus_dev_en[corpus_dev_en['AG'] != 0]\n",
    "text2 = aggressive_dev[aggressive_dev.columns[1]]\n",
    "lista2=lista(text2)\n",
    "a = set(lista1)\n",
    "b = set(lista2)\n",
    "d = a | b\n",
    "\n",
    "#sacar todos los hashtag\n",
    "c | d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesando el corpus B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus B\n",
    "corpus_train_enB = pd.read_csv('corpus/public_development_en_TaskB/train_en.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enB = pd.read_csv('corpus/public_development_en_TaskB/dev_en.tsv',delimiter='\\t',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardar texto medio limpio para sacar etiquetas POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesarme(file, namefile):    \n",
    "    file[file.columns[1]] = [clean(i) for i in file[file.columns[1]]]    \n",
    "    file.to_csv(namefile, sep='\\t', encoding='utf-8', index=False)\n",
    "    return\n",
    "\n",
    "def clean(text):\n",
    "    #text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \"@USER\", text)\n",
    "    text=re.sub(\"@([A-Za-z0-9_]{1,15})\", \" \", text)\n",
    "    text=re.sub(pattern_URL, \" \", text)\n",
    "    text= remove_emoji(text)\n",
    "    \n",
    "    #estandarizar\n",
    "    text=re.sub(\"['|´]\", \"’\", text)\n",
    "    \n",
    "    text= replace_all('Dictionary/EN/ENabb.txt', text)      \n",
    "    #text= replace_all('Dictionary/EN/ENslang.txt', text)\n",
    "    text= replace_all('Dictionary/EN/ENcontractions.txt', text)\n",
    "        \n",
    "    text= change_hashtag(text)\n",
    "    text=re.sub(\"(?:&gt|¤|ð|ÿ|‡|¨|¦|®)\", \" \", text) \n",
    "    text=re.sub(\"&amp\", \" and \", text) \n",
    "    text=re.sub(\"&\", \" and \", text)\n",
    "    text=re.sub(r\" +\", \" \", re.sub(r\"\\t\", \" \", re.sub(r\"\\n+\", \"\\n\", text)))\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "#Guardando el corpus\n",
    "procesarme(corpus_train_enB, \"corpus/public_development_en_TaskB/train_en_cPOSB.tsv\")\n",
    "procesarme(corpus_dev_enB, \"corpus/public_development_en_TaskB/dev_en_cPOSB.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar el texto limpio B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardando el corpus ya procesado B\n",
    "procesar(corpus_train_enB, \"corpus/public_development_en_TaskB/train_en_cleanB.tsv\")\n",
    "procesar(corpus_dev_enB, \"corpus/public_development_en_TaskB/dev_en_cleanB.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesando el corpus limpio B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leyendo el corpus ya procesado super limpio B\n",
    "corpus_train_enB = pd.read_csv('corpus/public_development_en_TaskB/train_en_cleanB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enB = pd.read_csv('corpus/public_development_en_TaskB/dev_en_cleanB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "\n",
    "#corpus_train_esB[corpus_train_esB.columns[1]]\n",
    "\n",
    "train_idB = corpus_train_enB[corpus_train_enB.columns[0]]\n",
    "X_train_textB = corpus_train_enB[corpus_train_enB.columns[1]].fillna(' ')\n",
    "y_train_hsB = corpus_train_enB[corpus_train_enB.columns[2]]\n",
    "y_train_trB = corpus_train_enB[corpus_train_enB.columns[3]]\n",
    "y_train_agB = corpus_train_enB[corpus_train_enB.columns[4]]\n",
    "\n",
    "test_idB = corpus_dev_enB[corpus_train_enB.columns[0]]\n",
    "X_test_textB = corpus_dev_enB[corpus_dev_enB.columns[1]].fillna(' ')\n",
    "y_test_hsB = corpus_dev_enB[corpus_dev_enB.columns[2]]\n",
    "y_test_trB = corpus_dev_enB[corpus_dev_enB.columns[3]]\n",
    "y_test_agB = corpus_dev_enB[corpus_dev_enB.columns[4]]\n",
    "\n",
    "#leyendo el corpus medio limpio para extracción de otras caracts\n",
    "corpus_train_enCB = pd.read_csv('corpus/public_development_en_TaskB/train_en_cPOSB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enCB = pd.read_csv('corpus/public_development_en_TaskB/dev_en_cPOSB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_B = corpus_train_enCB[corpus_train_enCB.columns[1]].fillna(' ')\n",
    "test_B = corpus_dev_enCB[corpus_dev_enCB.columns[1]].fillna(' ')\n",
    "\n",
    "#leyendo el corpus etiqueta POS\n",
    "corpus_train_enPOSB = pd.read_csv('corpus/public_development_en_TaskB/train_en_cPOSTAGB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "corpus_dev_enPOSB = pd.read_csv('corpus/public_development_en_TaskB/dev_en_cPOSTAGB.tsv',delimiter='\\t',encoding='utf-8')\n",
    "train_posB = corpus_train_enPOSB[corpus_train_enPOSB.columns[1]].fillna(' ')\n",
    "test_posB = corpus_dev_enPOSB[corpus_dev_enPOSB.columns[1]].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Procesar los corpus_train que sean hs\n",
    "hate=corpus_train_enB[corpus_train_enB['HS'] != 0]\n",
    "\n",
    "X_train_hs_textB = hate[hate.columns[1]].fillna(' ')\n",
    "y_train_hs_agB = hate[hate.columns[4]]\n",
    "\n",
    "#Procesar los corpus_c que sean hs\n",
    "hate_c=corpus_train_enCB[corpus_train_enCB['HS'] != 0]\n",
    "train_hs_B = hate_c[hate_c.columns[1]].fillna(' ')\n",
    "\n",
    "#Procesar los corpus_pos que sean hs\n",
    "hate_pos=corpus_train_enPOSB[corpus_train_enPOSB['HS'] != 0]\n",
    "train_hs_posB = hate_pos[hate_pos.columns[1]].fillna(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts character n-grams\n",
    "def charNgrams(text, n):\n",
    "    ngrams = []\n",
    "    ngrams = [text[i:i+n]+'_cng' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts word-ngrams, when n=1 is equal to bag of words\n",
    "def wordNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracts pos-ngrams, when n=1 is equal to bag of pos\n",
    "def posNgrams(text, n):\n",
    "    ngrams = []\n",
    "    text = [pos for pos in text.split()]\n",
    "    ngrams = [' '.join(text[i:i+n])+'_png' for i in range(len(text)-n+1)]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunto_agresivas = set()\n",
    "words_agresiva = open('Dictionary/agresivas_en.txt', 'r', encoding=\"utf8\")\n",
    "words_agresiva.seek(0)\n",
    "words_agresiva = words_agresiva.read().splitlines()\n",
    "ps = PorterStemmer()\n",
    "for agresiva in words_agresiva:\n",
    "    conjunto_agresivas.add(ps.stem(agresiva))\n",
    "    \n",
    "def AggressiveNgrams(text, n):\n",
    "    n_grams = []\n",
    "    tokens = text.split(\" \")\n",
    "    fws = []\n",
    "    for word in tokens:\n",
    "        if ps.stem(word) in conjunto_agresivas:\n",
    "            fws.append(word)\n",
    "    n_grams=[('_'.join(fws[i:i+n])) + \"_awn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams\n",
    "\n",
    "def lexPatterns(text):\n",
    "    patterns=[]\n",
    "    #Extracts patterns\n",
    "    for word in words_agresiva:\n",
    "        w =  re.findall(word, text)\n",
    "        w = ['lex_patt' for p in w]\n",
    "        patterns.extend(w)   \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordSkipgrams(text,n):\n",
    "    skipgrams = []\n",
    "    text = [word for word in text.split()]\n",
    "    lista = list(nltk.skipgrams(text, 2, n))\n",
    "    skipgrams = [' '.join(i[0]+' '+ i[1])+'' for i in lista]\n",
    "    return skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morfoPatterns(text):\n",
    "    patterns=[]\n",
    "    #Extracts patterns\n",
    "    \n",
    "    Vb_adj = re.findall(r'vm..000 aq.....',text)\n",
    "    Vb_adj= ['morfo_patt' for p in Vb_adj]\n",
    "    patterns.extend(Vb_adj)\n",
    "    \n",
    "    adj_vb = re.findall(r'aq..... vm..000',text)\n",
    "    adj_vb= ['morfo_patt' for p in adj_vb]\n",
    "    patterns.extend(adj_vb)\n",
    "    \n",
    "    sust_adj = re.findall(r'n.0.000 aq.....',text)\n",
    "    sust_adj= ['morfo_patt' for p in sust_adj]\n",
    "    patterns.extend(sust_adj)\n",
    "    \n",
    "    adj_sust = re.findall(r'aq..... n.0.000',text)\n",
    "    adj_sust= ['morfo_patt' for p in adj_sust]\n",
    "    patterns.extend(adj_sust)\n",
    "    \n",
    "    pron_vb = re.findall(r'pd...... vm..000',text)\n",
    "    pron_vb= ['morfo_patt' for p in pron_vb]\n",
    "    patterns.extend(pron_vb)\n",
    "    \n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcNgrams(text, n):\n",
    "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "    patt=r'\\b(' + ('|'.join(re.escape(key) for key in stop_words)).lstrip('|') + r')\\b'\n",
    "    pattern = re.compile(patt)\n",
    "    text = re.sub(r'[.,\\/!$%?¿?!¡\\^&\\*;:{}=><\\-_`~()”“\"\\'\\|]*', \"\",text)\n",
    "    #text = re.sub(r\"[\" + punctuation + \"]*\", \"\", text)\n",
    "    terms = pattern.findall(text)\n",
    "    n_grams=[('_'.join(terms[i:i+n])) + \"_fwn\" for i in range(len(terms)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simbPunctNgrams(text, n):\n",
    "    simb_punt = '.,\\/!$%?¿!¡^&*;:{}=><-_`~()”“\\'\\|'\n",
    "    lis_character = list(text)\n",
    "    fws = []\n",
    "    for c in lis_character:\n",
    "        if c in simb_punt:\n",
    "            fws.append(c)\n",
    "    n_grams=[(' '.join(fws[i:i+n])) + \"_pwn\" for i in range(len(fws)-n+1)]\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text,pos,tfs,cn,wn,pn,an,hs_ag,tr,sn,fn,sp):\n",
    "    features = []\n",
    "    for n in cn:\n",
    "        if n != 0:\n",
    "            features.extend(charNgrams(text,n))\n",
    "    for n in wn:\n",
    "        if n != 0:\n",
    "            features.extend(wordNgrams(text,n))\n",
    "    for n in pn:\n",
    "        if n != 0:\n",
    "            features.extend(posNgrams(pos,n))\n",
    "    for n in an:\n",
    "        if n != 0:\n",
    "            features.extend(AggressiveNgrams(text,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(wordSkipgrams(text,n))\n",
    "    for n in fn:\n",
    "        if n!=0:\n",
    "            features.extend(funcNgrams(tfs,n))\n",
    "    for n in sn:\n",
    "        if n!=0:\n",
    "            features.extend(simbPunctNgrams(tfs,n))\n",
    "    \n",
    "    if hs_ag:\n",
    "        features.extend(lexPatterns(text))\n",
    "    if tr:\n",
    "        features.extend(morfoPatterns(text))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts all features in a set of 'texts' and return as a string separated with the simbol '&%$'\n",
    "def process_texts(texts,poss,textfs,cn,wn,pn,an,hs_ag,tr,sn,fn,sp):\n",
    "    occurrences=defaultdict(int)\n",
    "    featuresList=[]\n",
    "    featuresDict=Counter()\n",
    "    text_pos= list(zip(texts,poss,textfs))   \n",
    "    for (text,pos,tfs) in text_pos:\n",
    "        features=extract_features(text,pos,tfs,cn,wn,pn,an,hs_ag,tr,sn,fn,sp)\n",
    "        featuresDict.update(features)\n",
    "        featuresList.append('&%$'.join(features))\n",
    "    return featuresList, featuresDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - HS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorHS(cn, wn, pn, an, sn, fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textB, train_posB,train_B,cn,wn,pn,an, True, False,sn,fn,sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_hsB))\n",
    "    print('\\t', 'tweets', len(X_train_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_hsB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_hsB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_hsB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_hsB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_hsB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_hsB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_hsB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_hsB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_hsB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB,test_B,cn,wn,pn,an, True, False,sn,fn,sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_hsB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_hsB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_hsB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_hsB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_hsB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_hsB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_hsB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_hsB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_hsB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_hsB, predictionsReg))\n",
    "    \n",
    "    return predictionsMnB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 9000\n",
      "\t tweets 9000\n",
      "\t vocabulary size 706410\n",
      "\t class dictribution Counter({0: 5217, 1: 3783})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.6614852821137841\n",
      "10-Fold Cross-validation Linear SVC 0.6213122012126704\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 1000 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 1917.1540038585663\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.724\n",
      "\t SVC 0.65\n",
      " - F1\n",
      "\t MultinomialNB 0.6933333333333334\n",
      "\t SVC 0.5603015075376885\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsHS = clasificadorHS(cnvalues, wnvalues, pnvalues, anvalues,skipgrams, fngrams, spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - TR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorTR(cn, wn, pn, an, sn, fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textB, train_posB,train_B,cn,wn,pn,an, False, True,sn, fn, sp)\n",
    "    \n",
    "    #vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_trB))\n",
    "    print('\\t', 'tweets', len(X_train_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_trB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_trB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_trB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_trB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_trB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_trB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_trB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_trB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_trB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB,test_B,cn,wn,pn,an, False, True,sn, fn, sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_trB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_trB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_trB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_trB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_trB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_trB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_trB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_trB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_trB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_trB, predictionsReg))\n",
    "    \n",
    "    return predictionsSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 9000\n",
      "\t tweets 9000\n",
      "\t vocabulary size 705969\n",
      "\t class dictribution Counter({0: 7659, 1: 1341})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.22866050631730048\n",
      "10-Fold Cross-validation Linear SVC 0.3781410048944202\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 1000 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 614.1547794342041\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.799\n",
      "\t SVC 0.779\n",
      " - F1\n",
      "\t MultinomialNB 0.26373626373626374\n",
      "\t SVC 0.6235093696763202\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[0]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsTR = clasificadorTR(cnvalues, wnvalues, pnvalues, anvalues,skipgrams,fngrams,spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - AG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorAG(cn, wn, pn, an,sn,fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures = process_texts(X_train_textB, train_posB,train_B,cn,wn,pn,an, True, False,sn,fn, sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #vectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_agB))\n",
    "    print('\\t', 'tweets', len(X_train_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_agB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_agB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_agB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_agB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_agB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_agB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_agB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB, test_B,cn,wn,pn,an, True, False,sn,fn, sp)\n",
    "    \n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_agB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_agB, predictionsReg))\n",
    "    \n",
    "    return predictionsMnB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 9000\n",
      "\t tweets 9000\n",
      "\t vocabulary size 706410\n",
      "\t class dictribution Counter({0: 7441, 1: 1559})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.3876599636677991\n",
      "10-Fold Cross-validation Linear SVC 0.3493831895394009\n",
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 1000 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 1504.5408155918121\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.758\n",
      "\t SVC 0.783\n",
      " - F1\n",
      "\t MultinomialNB 0.45982142857142855\n",
      "\t SVC 0.4119241192411924\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsAG = clasificadorAG(cnvalues, wnvalues, pnvalues, anvalues,skipgrams,fngrams,spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clasificador B - AG (a partir del corpus train_HS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificadorAG_hs(cn, wn, pn, an,sn,fn, sp):\n",
    "    start_time = time.time()\n",
    "    print('Reading file') \n",
    "    \n",
    "    '''\n",
    "    vect = CountVectorizer(min_df=3, ngram_range=(2,5)).fit(X_train_textB)\n",
    "    vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train_textB)\n",
    "    X_train_vectorized = vect.transform(X_train_textB)\n",
    "    '''\n",
    "    \n",
    "    print(' - Extracting features')\n",
    "    train_features, dicOfFeatures =process_texts(X_train_hs_textB,train_hs_posB,train_hs_B,cn,wn,pn,an,True,False,sn,fn,sp)\n",
    "    \n",
    "    vectorizer = CountVectorizer(lowercase=False, min_df=3, tokenizer=lambda x: x.split('&%$'))\n",
    "    #ectorizer = TfidfVectorizer(lowercase=False, min_df=5, tokenizer=lambda x: x.split('&%$'))\n",
    "    X_train_vectorized = vectorizer.fit_transform(train_features)\n",
    "    X_train_vectorized = X_train_vectorized.astype(float)\n",
    "    print('\\t', 'labels', len(y_train_hs_agB))\n",
    "    print('\\t', 'tweets', len(X_train_hs_textB))\n",
    "    print('\\t', 'vocabulary size',len(dicOfFeatures))\n",
    "    print('\\t', 'class dictribution',Counter(y_train_hs_agB) )\n",
    "    \n",
    "    ###### Clasificador\n",
    "    print(' - Training Classifier')\n",
    "        \n",
    "    modelMnB=MultinomialNB()\n",
    "    modelSVC = SVC(C=10000, random_state=0)   \n",
    "    #modelLR = LogisticRegression(C=100)\n",
    "    #modelMLPC = MLPClassifier()\n",
    "    #modelReg = MLPRegressor()\n",
    "    \n",
    "    cvScoreMnb=cross_val_score(modelMnB, X_train_vectorized, y_train_hs_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Multinomial Naive Bayes',cvScoreMnb)\n",
    "    \n",
    "    cvScoreSVC=cross_val_score(modelSVC, X_train_vectorized, y_train_hs_agB, cv=10, scoring='f1').mean()\n",
    "    print('10-Fold Cross-validation Linear SVC',cvScoreSVC)\n",
    "    \n",
    "    #cvScoreLG=cross_val_score(modelLR, X_train_vectorized, y_train_hs_agB, cv=10, scoring='f1').mean()\n",
    "    #print('10-Fold Cross-validation Logistic Regression',cvScoreLG)\n",
    "    \n",
    "    ######Entrenar clasificador#########\n",
    "    \n",
    "    modelMnB.fit(X_train_vectorized, y_train_hs_agB) #ajusta al calificador    \n",
    "    modelSVC.fit(X_train_vectorized, y_train_hs_agB)      \n",
    "    #modelLR.fit(X_train_vectorized, y_train_hs_agB)\n",
    "    #modelMLPC.fit(X_train_vectorized, y_train_hs_agB) \n",
    "    #modelReg.fit(X_train_vectorized, y_train_hs_agB)\n",
    "    \n",
    "    ###### Test ########################\n",
    "    print ('Reading Test files')\n",
    "    \n",
    "    print(' - Extracting Test features')\n",
    "    #X_test_vectorized = vect.transform(X_test_textB)\n",
    "    test_features, dicOfFeaturesTest = process_texts(X_test_textB, test_posB,test_B,cn,wn,pn,an, True, False,sn,fn, sp)\n",
    "    X_test_vectorized = vectorizer.transform(test_features)\n",
    "    X_test_vectorized = X_test_vectorized.astype(float)\n",
    "    X_test_vectorized = preprocessing.Binarizer().fit_transform(X_test_vectorized)\n",
    "    print('\\t', len(X_test_textB), 'unknown texts')\n",
    "        \n",
    "    # Predicting Test\n",
    "    print(' - Predicting Test')\n",
    "    \n",
    "    predictionsMnB = modelMnB.predict(X_test_vectorized) #funcion para predecir\n",
    "    predictionsSVC = modelSVC.predict(X_test_vectorized)\n",
    "    #predictions = cross_val_predict(model, X_test_vectorized, cv=10) #probando validacion cruzada predict\n",
    "    #predictionsLR = modelLR.predict(X_test_vectorized)\n",
    "    #predictionsMPLC = modelMLPC.predict(X_test_vectorized)\n",
    "    #predictionsReg = modelReg.predict(X_test_vectorized)\n",
    "    #predictions = [round(w) for w in predictionsMPLC]\n",
    "    \n",
    "    print('elapsed time:', time.time() - start_time)\n",
    "    \n",
    "    ###### Evaluation metrics ########################\n",
    "    print('Evaluation metrics')\n",
    "    print(' - ACC')\n",
    "    print('\\t', 'MultinomialNB', accuracy_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', accuracy_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', accuracy_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', accuracy_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', accuracy_score(y_test_agB, predictionsReg))\n",
    "    print(' - F1')\n",
    "    print('\\t', 'MultinomialNB', f1_score(y_test_agB, predictionsMnB))\n",
    "    print('\\t', 'SVC', f1_score(y_test_agB, predictionsSVC))\n",
    "    #print('\\t', 'LogisticRegression', f1_score(y_test_agB, predictionsLR))\n",
    "    #print('\\t', 'MLPClassifier', f1_score(y_test_agB, predictionsMPLC))\n",
    "    #print('\\t', 'MLPRegressor', f1_score(y_test_agB, predictionsReg))\n",
    "    \n",
    "    return predictionsMnB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file\n",
      " - Extracting features\n",
      "\t labels 3783\n",
      "\t tweets 3783\n",
      "\t vocabulary size 320907\n",
      "\t class dictribution Counter({0: 2224, 1: 1559})\n",
      " - Training Classifier\n",
      "10-Fold Cross-validation Multinomial Naive Bayes 0.5065518544364344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-validation Linear SVC 0.5089456798792859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Test files\n",
      " - Extracting Test features\n",
      "\t 1000 unknown texts\n",
      " - Predicting Test\n",
      "elapsed time: 213.2478952407837\n",
      "Evaluation metrics\n",
      " - ACC\n",
      "\t MultinomialNB 0.549\n",
      "\t SVC 0.638\n",
      " - F1\n",
      "\t MultinomialNB 0.3298662704309064\n",
      "\t SVC 0.4065573770491803\n"
     ]
    }
   ],
   "source": [
    "cnvalues=[3,4,5]#character n-grams\n",
    "wnvalues=[1,2,3]# word n-grams\n",
    "pnvalues=[2,3]#  pos n-grams\n",
    "anvalues=[2]# aggressive words n-grams\n",
    "skipgrams=[2,3,4] #skipgrams n-grams\n",
    "fngrams=[3,4] # stop words n-grams\n",
    "spgrams=[3,4] #punctuacion simbol n-gramas\n",
    "\n",
    "predictionsAG = clasificadorAG_hs(cnvalues, wnvalues, pnvalues, anvalues,skipgrams,fngrams,spgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función para el archivo de salida B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_tsv(testid, predictionsHS, predictionsTR, predictionsAG):    \n",
    "    d = {'id': testid, 'HS': predictionsHS, 'TR': predictionsTR, 'AG': predictionsAG}\n",
    "    file = pd.DataFrame(data=d)  \n",
    "    file.to_csv('corpus/public_development_en_TaskB/en_b.tsv', sep='\\t', encoding='utf-8', index=False)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output file\n",
      "- File created... answers saved to file: corpus/public_development_en_TaskB/en_b.tsv\n"
     ]
    }
   ],
   "source": [
    "###### File output ########################\n",
    "print('Writing output file')\n",
    "output_tsv(test_idB, predictionsHS, predictionsTR, predictionsAG)\n",
    "print('- File created...', 'answers saved to file:','corpus/public_development_en_TaskB/en_b.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
